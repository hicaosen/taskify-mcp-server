"""Taskify MCP Server - æ™ºèƒ½åŒ–ç¼–ç¨‹æ€ç»´å¯¼å¸ˆ"""

import re
import json
import time
import hashlib
from typing import Dict, List, Optional, Any
from enum import Enum
# åœ¨åŸæœ‰å‡½æ•°åŸºç¡€ä¸Šç§»é™¤é‡å¤çš„å¯¼å…¥
from dataclasses import dataclass
from mcp.server.fastmcp import FastMCP

mcp = FastMCP("taskify")

# å…¨å±€ä¼šè¯çŠ¶æ€ç®¡ç†
_session_cache = {}
_context_memory = {}  # ä¸Šä¸‹æ–‡è®°å¿†ç³»ç»Ÿ
_analysis_history = []  # åˆ†æå†å²è®°å½•

# ä¼šè¯æ¸…ç†é…ç½®
SESSION_TIMEOUT = 3600  # 1å°æ—¶è¶…æ—¶
MAX_SESSIONS = 100  # æœ€å¤§ç¼“å­˜ä¼šè¯æ•°


class TaskType(Enum):
    """ä»»åŠ¡ç±»å‹æšä¸¾"""
    NEW_FEATURE = "new_feature"
    BUG_FIX = "bug_fix"
    REFACTOR = "refactor"
    PERFORMANCE = "performance"
    TESTING = "testing"
    DOCUMENTATION = "documentation"
    MAINTENANCE = "maintenance"
    UNKNOWN = "unknown"


class ComplexityLevel(Enum):
    """å¤æ‚åº¦çº§åˆ«æšä¸¾"""
    SIMPLE = "simple"
    MEDIUM = "medium"
    COMPLEX = "complex"


@dataclass
class TaskAnalysis:
    """ä»»åŠ¡åˆ†æç»“æœ"""
    task_type: TaskType
    complexity_level: ComplexityLevel
    core_objective: str
    key_requirements: List[str]
    constraints: List[str]
    risk_factors: List[str]
    success_criteria: List[str]
    context_needs: List[str]
    similarity_score: float = 0.0  # ä¸å†å²ä»»åŠ¡çš„ç›¸ä¼¼åº¦
    learning_insights: Optional[List[str]] = None  # ä»å†å²ä¸­å­¦åˆ°çš„è§è§£


@dataclass
class ThinkingFramework:
    """æ€è€ƒæ¡†æ¶"""
    phase: str
    guiding_questions: List[str]
    key_considerations: List[str]
    output_format: str
    examples: List[str]
    adaptive_hints: Optional[List[str]] = None  # è‡ªé€‚åº”æç¤º


@dataclass
class SessionInfo:
    """ä¼šè¯ä¿¡æ¯"""
    session_id: str
    timestamp: float
    user_request: str
    project_context: str
    task_analysis: TaskAnalysis
    thinking_frameworks: Dict[str, ThinkingFramework]
    current_stage: str = "understanding"
    stage_history: Optional[List[str]] = None
    quality_scores: Optional[Dict[str, float]] = None

    def __post_init__(self):
        """åˆå§‹åŒ–å¯é€‰å­—æ®µçš„é»˜è®¤å€¼"""
        if self.stage_history is None:
            self.stage_history = []
        if self.quality_scores is None:
            self.quality_scores = {}


def generate_session_id(user_request: str) -> str:
    """ç”Ÿæˆå”¯ä¸€ä¼šè¯ID"""
    timestamp = str(time.time())
    content_hash = hashlib.md5(user_request.encode()).hexdigest()[:8]
    return f"session_{content_hash}_{int(float(timestamp))}"


def cleanup_expired_sessions():
    """æ¸…ç†è¿‡æœŸä¼šè¯"""
    current_time = time.time()
    expired_sessions = [
        sid for sid, session in _session_cache.items()
        if current_time - session.timestamp > SESSION_TIMEOUT
    ]
    
    for sid in expired_sessions:
        del _session_cache[sid]
    
    # å¦‚æœä¼šè¯æ•°é‡è¶…è¿‡é™åˆ¶ï¼Œåˆ é™¤æœ€æ—§çš„ä¼šè¯
    if len(_session_cache) > MAX_SESSIONS:
        sorted_sessions = sorted(_session_cache.items(), key=lambda x: x[1].timestamp)
        sessions_to_remove = sorted_sessions[:len(_session_cache) - MAX_SESSIONS]
        for sid, _ in sessions_to_remove:
            del _session_cache[sid]


def find_similar_tasks(user_request: str) -> List[Dict[str, Any]]:
    """ä»å†å²ä¸­æ‰¾åˆ°ç›¸ä¼¼çš„ä»»åŠ¡"""
    current_keywords = set(re.findall(r'\w+', user_request.lower()))
    similarities = []
    
    for history_item in _analysis_history:
        history_keywords = set(re.findall(r'\w+', history_item['user_request'].lower()))
        
        # è®¡ç®—å…³é”®è¯é‡å åº¦
        if current_keywords and history_keywords:
            overlap = len(current_keywords & history_keywords)
            similarity = overlap / len(current_keywords | history_keywords)
            
            if similarity > 0.3:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                similarities.append({
                    'similarity': similarity,
                    'task_type': history_item['task_type'],
                    'complexity': history_item['complexity'],
                    'lessons_learned': history_item.get('lessons_learned', [])
                })
    
    return sorted(similarities, key=lambda x: x['similarity'], reverse=True)[:3]


def analyze_task_type(user_request: str) -> TaskType:
    """åŸºäºç”¨æˆ·è¯·æ±‚åˆ†æä»»åŠ¡ç±»å‹ - å¢å¼ºç‰ˆ"""
    request_lower = user_request.lower()
    
    # å¢å¼ºçš„å…³é”®è¯åŒ¹é…è§„åˆ™ï¼ŒåŒ…å«æ›´å¤šä¸Šä¸‹æ–‡çº¿ç´¢
    type_keywords = {
        TaskType.NEW_FEATURE: [
            "add", "implement", "create", "build", "develop", "æ–°å¢", "æ·»åŠ ", "å®ç°", "æ„å»º",
            "feature", "functionality", "capability", "åŠŸèƒ½", "èƒ½åŠ›"
        ],
        TaskType.BUG_FIX: [
            "fix", "bug", "error", "issue", "problem", "ä¿®å¤", "é”™è¯¯", "é—®é¢˜", "æ•…éšœ",
            "broken", "crash", "fail", "exception", "å´©æºƒ", "å¤±è´¥", "å¼‚å¸¸"
        ],
        TaskType.REFACTOR: [
            "refactor", "restructure", "reorganize", "clean", "é‡æ„", "é‡ç»„", "æ¸…ç†",
            "improve", "simplify", "optimize code", "æ”¹è¿›", "ç®€åŒ–", "ä»£ç ä¼˜åŒ–"
        ],
        TaskType.PERFORMANCE: [
            "optimize", "performance", "speed", "memory", "efficient", "ä¼˜åŒ–", "æ€§èƒ½", "æ•ˆç‡",
            "slow", "fast", "latency", "throughput", "ç¼“æ…¢", "å»¶è¿Ÿ", "ååé‡"
        ],
        TaskType.TESTING: [
            "test", "testing", "unit test", "coverage", "æµ‹è¯•", "å•å…ƒæµ‹è¯•",
            "validate", "verify", "check", "éªŒè¯", "æ£€æŸ¥"
        ],
        TaskType.DOCUMENTATION: [
            "document", "doc", "readme", "comment", "æ–‡æ¡£", "æ³¨é‡Š",
            "explain", "describe", "guide", "è§£é‡Š", "æè¿°", "æŒ‡å—"
        ],
        TaskType.MAINTENANCE: [
            "update", "upgrade", "maintain", "dependency", "æ›´æ–°", "å‡çº§", "ç»´æŠ¤",
            "migrate", "deprecated", "è¿ç§»", "åºŸå¼ƒ"
        ]
    }
    
    # è®¡ç®—åŒ¹é…åˆ†æ•°è€Œä¸æ˜¯ç®€å•åŒ¹é…
    type_scores = {}
    for task_type, keywords in type_keywords.items():
        score = sum(2 if keyword in request_lower else 0 for keyword in keywords)
        
        # ä¸Šä¸‹æ–‡åŠ æƒï¼šæ£€æŸ¥å…³é”®è¯çš„ä¸Šä¸‹æ–‡
        for keyword in keywords:
            if keyword in request_lower:
                # æ£€æŸ¥å…³é”®è¯å‰åçš„ä¿®é¥°è¯
                context_words = re.findall(rf'\w*{keyword}\w*', request_lower)
                for context in context_words:
                    if any(modifier in context for modifier in ['new', 'better', 'improved']):
                        score += 1
        
        type_scores[task_type] = score
    
    # è¿”å›å¾—åˆ†æœ€é«˜çš„ä»»åŠ¡ç±»å‹
    if type_scores:
        best_type = max(type_scores.items(), key=lambda x: x[1])
        return best_type[0] if best_type[1] > 0 else TaskType.UNKNOWN
    
    return TaskType.UNKNOWN


def estimate_complexity(user_request: str, task_type: TaskType, project_context: str = "") -> ComplexityLevel:
    """è¯„ä¼°ä»»åŠ¡å¤æ‚åº¦ - æ™ºèƒ½å¢å¼ºç‰ˆ"""
    request_lower = user_request.lower()
    context_lower = project_context.lower()
    
    complexity_indicators = {
        "high": [
            "architecture", "system", "multiple", "integrate", "database", "api", 
            "microservice", "distributed", "æ¶æ„", "ç³»ç»Ÿ", "å¤šä¸ª", "é›†æˆ", "å¾®æœåŠ¡", "åˆ†å¸ƒå¼",
            "scalable", "enterprise", "production", "å¯æ‰©å±•", "ä¼ä¸šçº§", "ç”Ÿäº§ç¯å¢ƒ"
        ],
        "medium": [
            "module", "class", "function", "component", "service", "æ¨¡å—", "ç»„ä»¶", "ç±»", "å‡½æ•°", "æœåŠ¡",
            "interface", "workflow", "process", "æ¥å£", "å·¥ä½œæµ", "æµç¨‹"
        ],
        "low": [
            "variable", "config", "simple", "single", "basic", "å˜é‡", "é…ç½®", "ç®€å•", "å•ä¸ª", "åŸºç¡€",
            "small", "minor", "quick", "å°", "è½»å¾®", "å¿«é€Ÿ"
        ]
    }
    
    # è®¡ç®—å¤æ‚åº¦åˆ†æ•°
    high_score = sum(1 for keyword in complexity_indicators["high"] if keyword in request_lower or keyword in context_lower)
    medium_score = sum(1 for keyword in complexity_indicators["medium"] if keyword in request_lower or keyword in context_lower)
    low_score = sum(1 for keyword in complexity_indicators["low"] if keyword in request_lower or keyword in context_lower)
    
    # ä»»åŠ¡ç±»å‹çš„åŸºç¡€å¤æ‚åº¦ï¼ˆè°ƒæ•´åï¼‰
    base_complexity = {
        TaskType.NEW_FEATURE: 2,
        TaskType.REFACTOR: 2,
        TaskType.PERFORMANCE: 3,  # æ€§èƒ½ä¼˜åŒ–é€šå¸¸æ›´å¤æ‚
        TaskType.BUG_FIX: 1,
        TaskType.TESTING: 1,
        TaskType.DOCUMENTATION: 1,
        TaskType.MAINTENANCE: 1,
        TaskType.UNKNOWN: 1
    }
    
    # é¡¹ç›®ä¸Šä¸‹æ–‡å¤æ‚åº¦è°ƒæ•´
    context_complexity = 0
    if any(tech in context_lower for tech in ["react", "vue", "angular", "kubernetes", "docker"]):
        context_complexity += 1
    if any(scale in context_lower for scale in ["large", "enterprise", "distributed", "å¤§å‹", "ä¼ä¸š", "åˆ†å¸ƒå¼"]):
        context_complexity += 2
    
    total_score = (high_score * 3 + medium_score * 2 + low_score * 1 + 
                   base_complexity[task_type] + context_complexity)
    
    # åŠ¨æ€é˜ˆå€¼è°ƒæ•´
    if total_score >= 6:
        return ComplexityLevel.COMPLEX
    elif total_score >= 3:
        return ComplexityLevel.MEDIUM
    else:
        return ComplexityLevel.SIMPLE


def generate_thinking_framework(task_analysis: TaskAnalysis, similar_tasks: Optional[List[Dict]] = None) -> Dict[str, ThinkingFramework]:
    """æ ¹æ®ä»»åŠ¡åˆ†æç”Ÿæˆå®šåˆ¶åŒ–æ€è€ƒæ¡†æ¶ - æ™ºèƒ½å¢å¼ºç‰ˆ"""
    
    frameworks = {}
    
    # ä»ç›¸ä¼¼ä»»åŠ¡ä¸­å­¦ä¹ 
    adaptive_hints = []
    if similar_tasks:
        for similar in similar_tasks:
            if similar.get('lessons_learned'):
                adaptive_hints.extend(similar['lessons_learned'])
    
    # ç¬¬ä¸€é˜¶æ®µï¼šç†è§£é˜¶æ®µ
    frameworks["understanding"] = ThinkingFramework(
        phase="æ·±åº¦ç†è§£",
        guiding_questions=generate_understanding_questions(task_analysis),
        key_considerations=generate_understanding_considerations(task_analysis),
        output_format="é—®é¢˜æœ¬è´¨ã€ç”¨æˆ·æ„å›¾ã€éšå«éœ€æ±‚",
        examples=generate_understanding_examples(task_analysis),
        adaptive_hints=adaptive_hints[:2] if adaptive_hints else []
    )
    
    # ç¬¬äºŒé˜¶æ®µï¼šè§„åˆ’é˜¶æ®µ
    frameworks["planning"] = ThinkingFramework(
        phase="ç­–ç•¥è§„åˆ’",
        guiding_questions=generate_planning_questions(task_analysis),
        key_considerations=generate_planning_considerations(task_analysis),
        output_format="å®ç°è·¯å¾„ã€æŠ€æœ¯é€‰å‹ã€é£é™©è¯„ä¼°",
        examples=generate_planning_examples(task_analysis),
        adaptive_hints=get_planning_hints(task_analysis, similar_tasks)
    )
    
    # ç¬¬ä¸‰é˜¶æ®µï¼šå®ç°é˜¶æ®µ
    frameworks["implementation"] = ThinkingFramework(
        phase="ç²¾å‡†å®ç°",
        guiding_questions=generate_implementation_questions(task_analysis),
        key_considerations=generate_implementation_considerations(task_analysis),
        output_format="å…·ä½“æ­¥éª¤ã€ä»£ç ç»“æ„ã€æ¥å£è®¾è®¡",
        examples=generate_implementation_examples(task_analysis),
        adaptive_hints=get_implementation_hints(task_analysis)
    )
    
    # ç¬¬å››é˜¶æ®µï¼šéªŒè¯é˜¶æ®µ
    frameworks["validation"] = ThinkingFramework(
        phase="è´¨é‡éªŒè¯",
        guiding_questions=generate_validation_questions(task_analysis),
        key_considerations=generate_validation_considerations(task_analysis),
        output_format="æµ‹è¯•ç­–ç•¥ã€éªŒæ”¶æ ‡å‡†ã€æ€§èƒ½æŒ‡æ ‡",
        examples=generate_validation_examples(task_analysis),
        adaptive_hints=get_validation_hints(task_analysis)
    )
    
    return frameworks


def get_planning_hints(task_analysis: TaskAnalysis, similar_tasks: Optional[List[Dict]] = None) -> List[str]:
    """è·å–è§„åˆ’é˜¶æ®µçš„è‡ªé€‚åº”æç¤º"""
    hints = []
    
    # åŸºäºä»»åŠ¡ç±»å‹çš„ç‰¹å®šæç¤º
    if task_analysis.task_type == TaskType.NEW_FEATURE:
        hints.append("è€ƒè™‘åŠŸèƒ½çš„æ¸è¿›å¼å‘å¸ƒç­–ç•¥")
        hints.append("è®¾è®¡æ—¶ä¼˜å…ˆè€ƒè™‘ç”¨æˆ·ä½“éªŒå’Œæ€§èƒ½")
    elif task_analysis.task_type == TaskType.PERFORMANCE:
        hints.append("å»ºç«‹æ€§èƒ½åŸºçº¿ï¼Œé‡åŒ–ä¼˜åŒ–ç›®æ ‡")
        hints.append("è€ƒè™‘ç¼“å­˜ã€ç´¢å¼•ã€ç®—æ³•ä¼˜åŒ–ç­‰å¤šä¸ªå±‚é¢")
    elif task_analysis.task_type == TaskType.REFACTOR:
        hints.append("ç¡®ä¿é‡æ„çš„å‘åå…¼å®¹æ€§")
        hints.append("åˆ¶å®šè¯¦ç»†çš„æµ‹è¯•è®¡åˆ’ä»¥éªŒè¯é‡æ„æ•ˆæœ")
    
    # åŸºäºå¤æ‚åº¦çš„æç¤º
    if task_analysis.complexity_level == ComplexityLevel.COMPLEX:
        hints.append("å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºç‹¬ç«‹çš„å­ä»»åŠ¡")
        hints.append("è€ƒè™‘å¹¶è¡Œå¼€å‘å’Œé›†æˆç­–ç•¥")
    
    # ä»ç›¸ä¼¼ä»»åŠ¡ä¸­å­¦ä¹ 
    if similar_tasks:
        for similar in similar_tasks:
            if similar.get('lessons_learned'):
                hints.extend(similar['lessons_learned'][:1])
    
    return hints[:4]  # é™åˆ¶æç¤ºæ•°é‡


def get_implementation_hints(task_analysis: TaskAnalysis) -> List[str]:
    """è·å–å®ç°é˜¶æ®µçš„è‡ªé€‚åº”æç¤º"""
    hints = []
    
    if task_analysis.task_type == TaskType.NEW_FEATURE:
        hints.extend([
            "é‡‡ç”¨TDD(æµ‹è¯•é©±åŠ¨å¼€å‘)æ–¹æ³•",
            "å®ç°MVP(æœ€å°å¯è¡Œäº§å“)ç‰ˆæœ¬ï¼Œç„¶åè¿­ä»£"
        ])
    elif task_analysis.task_type == TaskType.BUG_FIX:
        hints.extend([
            "å…ˆé‡ç°é—®é¢˜ï¼Œå†å®šä½æ ¹å› ",
            "ä¿®å¤åæ·»åŠ å›å½’æµ‹è¯•é˜²æ­¢é—®é¢˜å†ç°"
        ])
    elif task_analysis.task_type == TaskType.PERFORMANCE:
        hints.extend([
            "ä½¿ç”¨æ€§èƒ½åˆ†æå·¥å…·å®šä½ç“¶é¢ˆ",
            "ä¼˜åŒ–å‰åè¿›è¡Œæ€§èƒ½å¯¹æ¯”æµ‹è¯•"
        ])
    
    return hints


def get_validation_hints(task_analysis: TaskAnalysis) -> List[str]:
    """è·å–éªŒè¯é˜¶æ®µçš„è‡ªé€‚åº”æç¤º"""
    hints = []
    
    if task_analysis.complexity_level == ComplexityLevel.COMPLEX:
        hints.extend([
            "è¿›è¡Œåˆ†å±‚æµ‹è¯•ï¼šå•å…ƒæµ‹è¯•ã€é›†æˆæµ‹è¯•ã€ç³»ç»Ÿæµ‹è¯•",
            "è€ƒè™‘è´Ÿè½½æµ‹è¯•å’Œå‹åŠ›æµ‹è¯•"
        ])
    
    if task_analysis.task_type == TaskType.NEW_FEATURE:
        hints.extend([
            "è¿›è¡Œç”¨æˆ·éªŒæ”¶æµ‹è¯•(UAT)",
            "æ”¶é›†ç”¨æˆ·åé¦ˆå¹¶å‡†å¤‡è¿­ä»£"
        ])
    
    return hints


def generate_understanding_questions(task_analysis: TaskAnalysis) -> List[str]:
    """ç”Ÿæˆç†è§£é˜¶æ®µçš„æŒ‡å¯¼é—®é¢˜"""
    base_questions = [
        "ç”¨æˆ·çœŸæ­£æƒ³è¦è§£å†³ä»€ä¹ˆæ ¸å¿ƒé—®é¢˜ï¼Ÿ",
        "è¿™ä¸ªéœ€æ±‚èƒŒåçš„ä¸šåŠ¡ä»·å€¼æ˜¯ä»€ä¹ˆï¼Ÿ",
        "æœ‰å“ªäº›éšå«çš„çº¦æŸå’ŒæœŸæœ›ï¼Ÿ"
    ]
    
    type_specific_questions = {
        TaskType.NEW_FEATURE: [
            "è¿™ä¸ªåŠŸèƒ½å¦‚ä½•èå…¥ç°æœ‰ç³»ç»Ÿï¼Ÿ",
            "é¢„æœŸçš„ç”¨æˆ·ä½¿ç”¨åœºæ™¯æ˜¯ä»€ä¹ˆï¼Ÿ",
            "åŠŸèƒ½è¾¹ç•Œåœ¨å“ªé‡Œï¼Ÿ"
        ],
        TaskType.BUG_FIX: [
            "é—®é¢˜çš„æ ¹æœ¬åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ",
            "å½±å“èŒƒå›´æœ‰å¤šå¤§ï¼Ÿ",
            "å¦‚ä½•é¿å…ç±»ä¼¼é—®é¢˜å†æ¬¡å‡ºç°ï¼Ÿ"
        ],
        TaskType.REFACTOR: [
            "å½“å‰è®¾è®¡çš„ç—›ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
            "é‡æ„çš„æœ€ç»ˆç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ",
            "å¦‚ä½•ç¡®ä¿é‡æ„åçš„å‘åå…¼å®¹æ€§ï¼Ÿ"
        ],
        TaskType.PERFORMANCE: [
            "æ€§èƒ½ç“¶é¢ˆåœ¨å“ªé‡Œï¼Ÿ",
            "ç›®æ ‡æ€§èƒ½æŒ‡æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ",
            "ä¼˜åŒ–çš„æƒè¡¡å–èˆæ˜¯ä»€ä¹ˆï¼Ÿ"
        ]
    }
    
    return base_questions + type_specific_questions.get(task_analysis.task_type, [])


def generate_understanding_considerations(task_analysis: TaskAnalysis) -> List[str]:
    """ç”Ÿæˆç†è§£é˜¶æ®µçš„å…³é”®è€ƒè™‘ç‚¹"""
    base_considerations = [
        "åŒºåˆ†æ˜¾æ€§éœ€æ±‚å’Œéšæ€§éœ€æ±‚",
        "è¯†åˆ«æŠ€æœ¯çº¦æŸå’Œä¸šåŠ¡çº¦æŸ",
        "è¯„ä¼°å˜æ›´çš„å½±å“èŒƒå›´"
    ]
    
    complexity_considerations = {
        ComplexityLevel.SIMPLE: ["ç¡®ä¿ç†è§£å‡†ç¡®ï¼Œé¿å…è¿‡åº¦è®¾è®¡"],
        ComplexityLevel.MEDIUM: ["å¹³è¡¡åŠŸèƒ½å®Œæ•´æ€§å’Œå®ç°å¤æ‚åº¦"],
        ComplexityLevel.COMPLEX: ["ç³»ç»Ÿæ€§æ€è€ƒï¼Œè€ƒè™‘æ¶æ„å½±å“", "åˆ†é˜¶æ®µå®ç°ç­–ç•¥"]
    }
    
    return base_considerations + complexity_considerations[task_analysis.complexity_level]


def generate_understanding_examples(task_analysis: TaskAnalysis) -> List[str]:
    """ç”Ÿæˆç†è§£é˜¶æ®µçš„ç¤ºä¾‹"""
    examples = {
        TaskType.NEW_FEATURE: ["ç”¨æˆ·è¯´'æ·»åŠ æœç´¢åŠŸèƒ½' â†’ ç†è§£ä¸ºï¼šéœ€è¦ä»€ä¹ˆç±»å‹çš„æœç´¢ï¼Ÿå®æ—¶æœç´¢è¿˜æ˜¯æ‰¹é‡æœç´¢ï¼Ÿæœç´¢èŒƒå›´æ˜¯ä»€ä¹ˆï¼Ÿ"],
        TaskType.BUG_FIX: ["ç”¨æˆ·è¯´'ç™»å½•æœ‰é—®é¢˜' â†’ ç†è§£ä¸ºï¼šä»€ä¹ˆæƒ…å†µä¸‹å‡ºé”™ï¼Ÿé”™è¯¯ç°è±¡æ˜¯ä»€ä¹ˆï¼Ÿå½±å“æ‰€æœ‰ç”¨æˆ·è¿˜æ˜¯ç‰¹å®šç”¨æˆ·ï¼Ÿ"],
        TaskType.REFACTOR: ["ç”¨æˆ·è¯´'ä»£ç å¤ªä¹±äº†' â†’ ç†è§£ä¸ºï¼šå…·ä½“å“ªäº›éƒ¨åˆ†éœ€è¦é‡æ„ï¼Ÿé‡æ„çš„ä¼˜å…ˆçº§æ˜¯ä»€ä¹ˆï¼Ÿ"],
        TaskType.PERFORMANCE: ["ç”¨æˆ·è¯´'å¤ªæ…¢äº†' â†’ ç†è§£ä¸ºï¼šå“ªä¸ªç¯èŠ‚æ…¢ï¼Ÿå¯æ¥å—çš„å“åº”æ—¶é—´æ˜¯å¤šå°‘ï¼Ÿ"]
    }
    
    return examples.get(task_analysis.task_type, ["æ·±å…¥ç†è§£ç”¨æˆ·çœŸå®éœ€æ±‚ï¼Œè€Œéè¡¨é¢æè¿°"])


def generate_planning_questions(task_analysis: TaskAnalysis) -> List[str]:
    """ç”Ÿæˆè§„åˆ’é˜¶æ®µçš„æŒ‡å¯¼é—®é¢˜"""
    return [
        "æœ€ä½³çš„å®ç°è·¯å¾„æ˜¯ä»€ä¹ˆï¼Ÿ",
        "éœ€è¦å“ªäº›æŠ€æœ¯æ ˆå’Œå·¥å…·ï¼Ÿ",
        "å¦‚ä½•åˆ†è§£ä»»åŠ¡ä»¥é™ä½é£é™©ï¼Ÿ",
        "æœ‰å“ªäº›å¯èƒ½çš„æŠ€æœ¯é™·é˜±ï¼Ÿ",
        "å¦‚ä½•ç¡®ä¿ä»£ç è´¨é‡å’Œå¯ç»´æŠ¤æ€§ï¼Ÿ"
    ]


def generate_planning_considerations(task_analysis: TaskAnalysis) -> List[str]:
    """ç”Ÿæˆè§„åˆ’é˜¶æ®µçš„å…³é”®è€ƒè™‘ç‚¹"""
    base_considerations = [
        "é€‰æ‹©åˆé€‚çš„æŠ€æœ¯æ–¹æ¡ˆ",
        "è¯„ä¼°å¼€å‘æˆæœ¬å’Œæ—¶é—´",
        "è€ƒè™‘æœªæ¥æ‰©å±•æ€§"
    ]
    
    if task_analysis.complexity_level == ComplexityLevel.COMPLEX:
        base_considerations.extend([
            "è®¾è®¡ç³»ç»Ÿæ¶æ„",
            "å®šä¹‰æ¨¡å—æ¥å£",
            "åˆ¶å®šè¿­ä»£è®¡åˆ’"
        ])
    
    return base_considerations


def generate_planning_examples(task_analysis: TaskAnalysis) -> List[str]:
    """ç”Ÿæˆè§„åˆ’é˜¶æ®µçš„ç¤ºä¾‹"""
    return [
        "æŠ€æœ¯é€‰å‹ï¼šReact vs Vue â†’ è€ƒè™‘å›¢é˜ŸæŠ€èƒ½ã€é¡¹ç›®éœ€æ±‚ã€ç”Ÿæ€ç³»ç»Ÿ",
        "æ¶æ„è®¾è®¡ï¼šå•ä½“ vs å¾®æœåŠ¡ â†’ è€ƒè™‘é¡¹ç›®è§„æ¨¡ã€å›¢é˜Ÿèƒ½åŠ›ã€ç»´æŠ¤æˆæœ¬"
    ]


def generate_implementation_questions(task_analysis: TaskAnalysis) -> List[str]:
    """ç”Ÿæˆå®ç°é˜¶æ®µçš„æŒ‡å¯¼é—®é¢˜"""
    return [
        "å¦‚ä½•ç»„ç»‡ä»£ç ç»“æ„ï¼Ÿ",
        "æ¥å£è®¾è®¡æ˜¯å¦æ¸…æ™°åˆç†ï¼Ÿ",
        "é”™è¯¯å¤„ç†ç­–ç•¥æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å¦‚ä½•ç¡®ä¿ä»£ç çš„å¯æµ‹è¯•æ€§ï¼Ÿ",
        "æ˜¯å¦éµå¾ªäº†é¡¹ç›®çš„ç¼–ç è§„èŒƒï¼Ÿ"
    ]


def generate_implementation_considerations(task_analysis: TaskAnalysis) -> List[str]:
    """ç”Ÿæˆå®ç°é˜¶æ®µçš„å…³é”®è€ƒè™‘ç‚¹"""
    return [
        "ä¿æŒä»£ç ç®€æ´å’Œå¯è¯»æ€§",
        "éµå¾ªè®¾è®¡æ¨¡å¼å’Œæœ€ä½³å®è·µ",
        "è€ƒè™‘å¼‚å¸¸æƒ…å†µçš„å¤„ç†",
        "ç¡®ä¿æ¥å£çš„å‘åå…¼å®¹æ€§",
        "æ·»åŠ å¿…è¦çš„æ—¥å¿—å’Œç›‘æ§"
    ]


def generate_implementation_examples(task_analysis: TaskAnalysis) -> List[str]:
    """ç”Ÿæˆå®ç°é˜¶æ®µçš„ç¤ºä¾‹"""
    return [
        "å‡½æ•°è®¾è®¡ï¼šå•ä¸€èŒè´£ã€æ¸…æ™°å‘½åã€é€‚å½“æŠ½è±¡",
        "é”™è¯¯å¤„ç†ï¼šé¢„æœŸå¼‚å¸¸ vs æ„å¤–å¼‚å¸¸çš„ä¸åŒå¤„ç†ç­–ç•¥"
    ]


def generate_validation_questions(task_analysis: TaskAnalysis) -> List[str]:
    """ç”ŸæˆéªŒè¯é˜¶æ®µçš„æŒ‡å¯¼é—®é¢˜"""
    return [
        "å¦‚ä½•éªŒè¯åŠŸèƒ½çš„æ­£ç¡®æ€§ï¼Ÿ",
        "æ€§èƒ½æ˜¯å¦æ»¡è¶³è¦æ±‚ï¼Ÿ",
        "æ˜¯å¦è€ƒè™‘äº†è¾¹ç•Œæƒ…å†µï¼Ÿ",
        "ç”¨æˆ·ä½“éªŒæ˜¯å¦è‰¯å¥½ï¼Ÿ",
        "æ˜¯å¦æœ‰å……åˆ†çš„æµ‹è¯•è¦†ç›–ï¼Ÿ"
    ]


def generate_validation_considerations(task_analysis: TaskAnalysis) -> List[str]:
    """ç”ŸæˆéªŒè¯é˜¶æ®µçš„å…³é”®è€ƒè™‘ç‚¹"""
    return [
        "åŠŸèƒ½æµ‹è¯•å’Œé›†æˆæµ‹è¯•",
        "æ€§èƒ½åŸºå‡†æµ‹è¯•",
        "ç”¨æˆ·ä½“éªŒéªŒè¯",
        "ä»£ç è´¨é‡æ£€æŸ¥",
        "æ–‡æ¡£å®Œæ•´æ€§ç¡®è®¤"
    ]


def generate_validation_examples(task_analysis: TaskAnalysis) -> List[str]:
    """ç”ŸæˆéªŒè¯é˜¶æ®µçš„ç¤ºä¾‹"""
    return [
        "APIæµ‹è¯•ï¼šæ­£å¸¸æƒ…å†µã€å¼‚å¸¸æƒ…å†µã€è¾¹ç•Œæƒ…å†µ",
        "æ€§èƒ½æµ‹è¯•ï¼šå“åº”æ—¶é—´ã€å¹¶å‘å¤„ç†ã€å†…å­˜ä½¿ç”¨"
    ]


@mcp.tool()
def analyze_programming_context(
    user_request: str,
    project_context: str = "",
    complexity_hint: str = "auto"
) -> str:
    """
    ğŸ§  æ™ºèƒ½ç¼–ç¨‹ä»»åŠ¡åˆ†æå™¨ V2.0 - å¯å‘å¼æ€ç»´çš„èµ·ç‚¹
    
    **é‡å¤§å‡çº§ç‰¹æ€§ï¼š**
    â€¢ âœ¨ ä¼šè¯çŠ¶æ€ç®¡ç† - æ— éœ€ä¼ é€’å¤§JSONï¼Œä½¿ç”¨ç®€å•session_id
    â€¢ ğŸ§  æ™ºèƒ½å­¦ä¹ ç³»ç»Ÿ - ä»å†å²ä»»åŠ¡ä¸­å­¦ä¹ ï¼Œæä¾›ä¸ªæ€§åŒ–å»ºè®®
    â€¢ ğŸ¯ è‡ªé€‚åº”æ¡†æ¶ - æ ¹æ®ä»»åŠ¡ç‰¹ç‚¹åŠ¨æ€è°ƒæ•´æ€è€ƒæ¡†æ¶
    â€¢ ğŸ“Š ä¸Šä¸‹æ–‡è®°å¿† - è®°ä½é¡¹ç›®èƒŒæ™¯ï¼Œç´¯ç§¯æ™ºæ…§
    
    **æ ¸å¿ƒèƒ½åŠ›ï¼š**
    â€¢ è‡ªåŠ¨è¯†åˆ«ä»»åŠ¡ç±»å‹ï¼ˆæ–°åŠŸèƒ½ã€Bugä¿®å¤ã€æ€§èƒ½ä¼˜åŒ–ã€é‡æ„ç­‰ï¼‰
    â€¢ æ™ºèƒ½è¯„ä¼°å¤æ‚åº¦çº§åˆ«ï¼ˆç®€å•/ä¸­ç­‰/å¤æ‚ï¼‰
    â€¢ æä¾›åœºæ™¯åŒ–çš„4é˜¶æ®µæ€è€ƒæ¡†æ¶ï¼ˆç†è§£â†’è§„åˆ’â†’å®ç°â†’éªŒè¯ï¼‰
    â€¢ ç”Ÿæˆå®šåˆ¶åŒ–çš„æŒ‡å¯¼é—®é¢˜å’Œå…³é”®è€ƒè™‘ç‚¹
    â€¢ ä»ç›¸ä¼¼ä»»åŠ¡ä¸­å­¦ä¹ ï¼Œæä¾›æ™ºèƒ½å»ºè®®
    
    **ä½¿ç”¨åœºæ™¯ï¼š**
    - é¢å¯¹æ–°çš„ç¼–ç¨‹ä»»åŠ¡æ—¶ï¼Œä¸ç¡®å®šä»ä½•æ€è€ƒ
    - éœ€è¦ç³»ç»ŸåŒ–çš„æ€è€ƒæ¡†æ¶æ¥æŒ‡å¯¼ä»»åŠ¡åˆ†æ
    - å¸Œæœ›æ ¹æ®ä»»åŠ¡ç‰¹ç‚¹è·å¾—é’ˆå¯¹æ€§çš„æ€è€ƒæŒ‡å¯¼
    - æƒ³è¦ç¡®ä¿è€ƒè™‘åˆ°æ‰€æœ‰é‡è¦çš„æŠ€æœ¯å’Œä¸šåŠ¡å› ç´ 
    
    Args:
        user_request: ç”¨æˆ·çš„ç¼–ç¨‹è¯·æ±‚æè¿°
        project_context: é¡¹ç›®èƒŒæ™¯ä¿¡æ¯ï¼ˆæŠ€æœ¯æ ˆã€æ¶æ„çº¦æŸç­‰ï¼‰
        complexity_hint: å¤æ‚åº¦æç¤º ("simple"/"medium"/"complex"/"auto")
    
    Returns:
        è½»é‡çº§ä¼šè¯ä¿¡æ¯ï¼Œåç»­å·¥å…·ä½¿ç”¨session_idå³å¯ï¼š
        {
            "session_id": "session_abc123_1234567890",
            "task_summary": {
                "task_type": "ä»»åŠ¡ç±»å‹",
                "complexity_level": "å¤æ‚åº¦çº§åˆ«",
                "core_objective": "æ ¸å¿ƒç›®æ ‡"
            },
            "intelligent_insights": {
                "similarity_analysis": "ä¸å†å²ä»»åŠ¡çš„ç›¸ä¼¼åº¦åˆ†æ",
                "learning_suggestions": ["ä»ç›¸ä¼¼ä»»åŠ¡ä¸­å­¦åˆ°çš„å»ºè®®"],
                "risk_prediction": ["åŸºäºå†å²çš„é£é™©é¢„æµ‹"]
            },
            "next_steps": {
                "recommended_workflow": "æ¨èçš„æ€è€ƒæµç¨‹",
                "first_action": "å»ºè®®çš„ç¬¬ä¸€æ­¥è¡ŒåŠ¨"
            },
            "session_info": "ä¼šè¯å·²åˆ›å»ºï¼Œä½¿ç”¨session_idè¿›è¡Œåç»­æ€è€ƒæŒ‡å¯¼"
        }
    """
    
    # æ¸…ç†è¿‡æœŸä¼šè¯
    cleanup_expired_sessions()
    
    # åˆ†æä»»åŠ¡ç±»å‹
    task_type = analyze_task_type(user_request)
    
    # ä¼°ç®—å¤æ‚åº¦ï¼ˆå¢å¼ºç‰ˆï¼‰
    if complexity_hint == "auto":
        complexity_level = estimate_complexity(user_request, task_type, project_context)
    else:
        complexity_level = ComplexityLevel(complexity_hint)
    
    # æ™ºèƒ½ç›¸ä¼¼ä»»åŠ¡åˆ†æ
    similar_tasks = find_similar_tasks(user_request)
    similarity_score = similar_tasks[0]['similarity'] if similar_tasks else 0.0
    
    # ç”Ÿæˆä»»åŠ¡åˆ†æï¼ˆå¢å¼ºç‰ˆï¼‰
    task_analysis = TaskAnalysis(
        task_type=task_type,
        complexity_level=complexity_level,
        core_objective=extract_core_objective(user_request),
        key_requirements=extract_requirements(user_request),
        constraints=extract_constraints(user_request, project_context),
        risk_factors=identify_risk_factors(user_request, task_type),
        success_criteria=define_success_criteria(user_request, task_type),
        context_needs=identify_context_needs(user_request, project_context),
        similarity_score=similarity_score,
        learning_insights=[task['lessons_learned'] for task in similar_tasks if task.get('lessons_learned')]
    )
    
    # ç”Ÿæˆæ™ºèƒ½æ€è€ƒæ¡†æ¶
    frameworks = generate_thinking_framework(task_analysis, similar_tasks)
    
    # åˆ›å»ºä¼šè¯
    session_id = generate_session_id(user_request)
    session_info = SessionInfo(
        session_id=session_id,
        timestamp=time.time(),
        user_request=user_request,
        project_context=project_context,
        task_analysis=task_analysis,
        thinking_frameworks=frameworks,
        current_stage="understanding",
        stage_history=[],
        quality_scores={}
    )
    
    # å­˜å‚¨ä¼šè¯çŠ¶æ€
    _session_cache[session_id] = session_info
    
    # æ›´æ–°ä¸Šä¸‹æ–‡è®°å¿†
    if project_context:
        context_key = hashlib.md5(project_context.encode()).hexdigest()[:8]
        _context_memory[context_key] = {
            'context': project_context,
            'timestamp': time.time(),
            'task_count': _context_memory.get(context_key, {}).get('task_count', 0) + 1
        }
    
    # æ·»åŠ åˆ°åˆ†æå†å²
    _analysis_history.append({
        'user_request': user_request,
        'task_type': task_type.value,
        'complexity': complexity_level.value,
        'timestamp': time.time(),
        'session_id': session_id
    })
    
    # é™åˆ¶å†å²è®°å½•å¤§å°
    if len(_analysis_history) > 50:
        _analysis_history.pop(0)
    
    # ç”Ÿæˆæ™ºèƒ½æ´å¯Ÿ
    intelligent_insights = {
        "similarity_analysis": f"å‘ç°{len(similar_tasks)}ä¸ªç›¸ä¼¼ä»»åŠ¡ï¼Œæœ€é«˜ç›¸ä¼¼åº¦{similarity_score:.2f}" if similar_tasks else "æœªå‘ç°ç›¸ä¼¼çš„å†å²ä»»åŠ¡",
        "learning_suggestions": [insight for task in similar_tasks for insight in task.get('lessons_learned', [])][:3],
        "risk_prediction": predict_risks_from_history(task_analysis, similar_tasks),
        "context_familiarity": f"é¡¹ç›®ä¸Šä¸‹æ–‡ç†Ÿæ‚‰åº¦: {get_context_familiarity(project_context)}/5"
    }
    
    # æ„å»ºè½»é‡çº§è¿”å›ç»“æœ
    result = {
        "session_id": session_id,
        "task_summary": {
            "task_type": task_analysis.task_type.value,
            "complexity_level": task_analysis.complexity_level.value,
            "core_objective": task_analysis.core_objective,
            "estimated_stages": len(frameworks)
        },
        "intelligent_insights": intelligent_insights,
        "next_steps": {
            "recommended_workflow": get_workflow_recommendation(complexity_level),
            "first_action": f"å¼€å§‹ guided_thinking_process('{session_id}', 'understanding')",
            "tools_sequence": ["guided_thinking_process"] * len(frameworks) + ["validate_instruction_quality"]
        },
        "session_info": f"âœ… ä¼šè¯å·²åˆ›å»ºï¼ŒID: {session_id}ã€‚ç°åœ¨å¯ä»¥ä½¿ç”¨session_idè¿›è¡Œåç»­æ€è€ƒæŒ‡å¯¼ï¼Œæ— éœ€ä¼ é€’å¤§JSONã€‚"
    }
    
    return json.dumps(result, ensure_ascii=False, indent=2)


def predict_risks_from_history(task_analysis: TaskAnalysis, similar_tasks: List[Dict]) -> List[str]:
    """åŸºäºå†å²ä»»åŠ¡é¢„æµ‹é£é™©"""
    risks = []
    
    # åŸºäºç›¸ä¼¼ä»»åŠ¡çš„é£é™©
    for similar in similar_tasks:
        if similar.get('common_risks'):
            risks.extend(similar['common_risks'])
    
    # åŸºäºä»»åŠ¡ç±»å‹çš„å¸¸è§é£é™©
    type_risks = {
        TaskType.NEW_FEATURE: ["éœ€æ±‚å˜æ›´é£é™©", "é›†æˆå¤æ‚åº¦é£é™©"],
        TaskType.PERFORMANCE: ["è¿‡åº¦ä¼˜åŒ–é£é™©", "å…¼å®¹æ€§é£é™©"],
        TaskType.REFACTOR: ["ç ´åç°æœ‰åŠŸèƒ½é£é™©", "èŒƒå›´æ‰©å¤§é£é™©"]
    }
    
    risks.extend(type_risks.get(task_analysis.task_type, []))
    
    return list(set(risks))[:3]  # å»é‡å¹¶é™åˆ¶æ•°é‡


def get_context_familiarity(project_context: str) -> int:
    """è·å–é¡¹ç›®ä¸Šä¸‹æ–‡ç†Ÿæ‚‰åº¦ï¼ˆ1-5åˆ†ï¼‰"""
    if not project_context:
        return 1
    
    context_key = hashlib.md5(project_context.encode()).hexdigest()[:8]
    memory = _context_memory.get(context_key, {})
    task_count = memory.get('task_count', 0)
    
    # åŸºäºå†å²ä»»åŠ¡æ•°é‡è¯„ä¼°ç†Ÿæ‚‰åº¦
    if task_count >= 10:
        return 5
    elif task_count >= 5:
        return 4
    elif task_count >= 3:
        return 3
    elif task_count >= 1:
        return 2
    else:
        return 1


def get_workflow_recommendation(complexity: ComplexityLevel) -> str:
    """è·å–å·¥ä½œæµæ¨è"""
    workflows = {
        ComplexityLevel.SIMPLE: "è½»é‡çº§æµç¨‹ï¼šunderstanding â†’ implementation â†’ validation",
        ComplexityLevel.MEDIUM: "æ ‡å‡†æµç¨‹ï¼šunderstanding â†’ planning â†’ implementation â†’ validation",
        ComplexityLevel.COMPLEX: "æ·±åº¦æµç¨‹ï¼šunderstanding â†’ planning â†’ implementation â†’ validation + å¯èƒ½çš„å¤šè½®è¿­ä»£"
    }
    return workflows[complexity]


def extract_core_objective(user_request: str) -> str:
    """æå–æ ¸å¿ƒç›®æ ‡"""
    # ç®€å•çš„ç›®æ ‡æå–é€»è¾‘
    if "implement" in user_request.lower() or "å®ç°" in user_request:
        return "å®ç°æ–°åŠŸèƒ½"
    elif "fix" in user_request.lower() or "ä¿®å¤" in user_request:
        return "ä¿®å¤é—®é¢˜"
    elif "optimize" in user_request.lower() or "ä¼˜åŒ–" in user_request:
        return "ä¼˜åŒ–æ€§èƒ½"
    elif "refactor" in user_request.lower() or "é‡æ„" in user_request:
        return "é‡æ„ä»£ç "
    else:
        return "å®Œæˆç¼–ç¨‹ä»»åŠ¡"


def extract_requirements(user_request: str) -> List[str]:
    """æå–å…³é”®éœ€æ±‚"""
    # ç®€åŒ–çš„éœ€æ±‚æå–
    requirements = []
    if "test" in user_request.lower() or "æµ‹è¯•" in user_request:
        requirements.append("åŒ…å«æµ‹è¯•ç”¨ä¾‹")
    if "document" in user_request.lower() or "æ–‡æ¡£" in user_request:
        requirements.append("æä¾›æ–‡æ¡£è¯´æ˜")
    if "performance" in user_request.lower() or "æ€§èƒ½" in user_request:
        requirements.append("è€ƒè™‘æ€§èƒ½ä¼˜åŒ–")
    
    return requirements if requirements else ["æ»¡è¶³åŸºæœ¬åŠŸèƒ½éœ€æ±‚"]


def extract_constraints(user_request: str, project_context: str) -> List[str]:
    """æå–çº¦æŸæ¡ä»¶"""
    constraints = []
    if "backward compatible" in user_request.lower() or "å‘åå…¼å®¹" in user_request:
        constraints.append("ä¿æŒå‘åå…¼å®¹æ€§")
    if project_context:
        constraints.append("éµå¾ªé¡¹ç›®ç°æœ‰æ¶æ„")
    
    return constraints if constraints else ["éµå¾ªç¼–ç¨‹æœ€ä½³å®è·µ"]


def identify_risk_factors(user_request: str, task_type: TaskType) -> List[str]:
    """è¯†åˆ«é£é™©å› ç´ """
    risk_factors = []
    
    if task_type == TaskType.NEW_FEATURE:
        risk_factors.extend(["åŠŸèƒ½èŒƒå›´è”“å»¶", "ä¸ç°æœ‰åŠŸèƒ½å†²çª"])
    elif task_type == TaskType.BUG_FIX:
        risk_factors.extend(["ä¿®å¤å¼•å…¥æ–°é—®é¢˜", "å½±å“å…¶ä»–åŠŸèƒ½"])
    elif task_type == TaskType.REFACTOR:
        risk_factors.extend(["ç ´åç°æœ‰åŠŸèƒ½", "é‡æ„èŒƒå›´è¿‡å¤§"])
    elif task_type == TaskType.PERFORMANCE:
        risk_factors.extend(["è¿‡åº¦ä¼˜åŒ–", "å¯è¯»æ€§ä¸‹é™"])
    
    return risk_factors


def define_success_criteria(user_request: str, task_type: TaskType) -> List[str]:
    """å®šä¹‰æˆåŠŸæ ‡å‡†"""
    base_criteria = ["åŠŸèƒ½æ­£ç¡®å®ç°", "ä»£ç è´¨é‡è‰¯å¥½", "é€šè¿‡æµ‹è¯•éªŒè¯"]
    
    type_specific_criteria = {
        TaskType.NEW_FEATURE: ["æ»¡è¶³ç”¨æˆ·éœ€æ±‚", "æ€§èƒ½è¡¨ç°è‰¯å¥½"],
        TaskType.BUG_FIX: ["é—®é¢˜å®Œå…¨è§£å†³", "æ— å‰¯ä½œç”¨"],
        TaskType.REFACTOR: ["ä»£ç æ›´æ¸…æ™°", "æ€§èƒ½ä¸é™ä½"],
        TaskType.PERFORMANCE: ["è¾¾åˆ°æ€§èƒ½ç›®æ ‡", "ä¿æŒåŠŸèƒ½å®Œæ•´"]
    }
    
    return base_criteria + type_specific_criteria.get(task_type, [])


def identify_context_needs(user_request: str, project_context: str) -> List[str]:
    """è¯†åˆ«ä¸Šä¸‹æ–‡éœ€æ±‚"""
    needs = ["äº†è§£ç°æœ‰ä»£ç ç»“æ„", "ç†è§£ä¸šåŠ¡é€»è¾‘"]
    
    if not project_context:
        needs.append("è·å–é¡¹ç›®æ¶æ„ä¿¡æ¯")
    
    return needs


def generate_approach_recommendation(task_analysis: TaskAnalysis) -> str:
    """ç”Ÿæˆå®ç°æ–¹æ³•å»ºè®®"""
    if task_analysis.complexity_level == ComplexityLevel.SIMPLE:
        return "ç›´æ¥å®ç°ï¼Œæ³¨é‡ä»£ç è´¨é‡"
    elif task_analysis.complexity_level == ComplexityLevel.MEDIUM:
        return "åˆ†æ­¥å®ç°ï¼Œå…ˆè®¾è®¡åç¼–ç "
    else:
        return "åˆ†é˜¶æ®µå®ç°ï¼Œå…ˆåˆ¶å®šè¯¦ç»†è®¡åˆ’"


def generate_quality_checklist(task_analysis: TaskAnalysis) -> List[str]:
    """ç”Ÿæˆè´¨é‡æ£€æŸ¥æ¸…å•"""
    return [
        "ä»£ç æ˜¯å¦æ¸…æ™°æ˜“è¯»ï¼Ÿ",
        "æ˜¯å¦éµå¾ªé¡¹ç›®è§„èŒƒï¼Ÿ",
        "é”™è¯¯å¤„ç†æ˜¯å¦å®Œå–„ï¼Ÿ",
        "æ˜¯å¦æœ‰å……åˆ†çš„æµ‹è¯•ï¼Ÿ",
        "æ€§èƒ½æ˜¯å¦å¯æ¥å—ï¼Ÿ",
        "æ˜¯å¦è€ƒè™‘äº†è¾¹ç•Œæƒ…å†µï¼Ÿ"
    ]


@mcp.tool()
def guided_thinking_process(
    session_id: str,
    current_step: str = "understanding"
) -> str:
    """
    ğŸ¯ æ¸è¿›å¼æ€è€ƒå¼•å¯¼å™¨ V2.0 - æ­¥æ­¥ä¸ºè¥çš„æ™ºæ…§è·¯å¾„
    
    **é‡å¤§å‡çº§ç‰¹æ€§ï¼š**
    â€¢ âœ¨ ä¼šè¯çŠ¶æ€é©±åŠ¨ - ä½¿ç”¨ç®€å•session_idï¼Œæ— éœ€ä¼ é€’å¤§JSON
    â€¢ ğŸ§  æ™ºèƒ½è¿›åº¦è·Ÿè¸ª - è‡ªåŠ¨è®°å½•æ€è€ƒå†ç¨‹ï¼Œæä¾›è¿è´¯æŒ‡å¯¼
    â€¢ ğŸ¯ è‡ªé€‚åº”æç¤º - åŸºäºå†å²å­¦ä¹ å’Œå½“å‰ä¸Šä¸‹æ–‡çš„ä¸ªæ€§åŒ–å»ºè®®
    â€¢ ğŸ“Š è´¨é‡åé¦ˆå¾ªç¯ - å®æ—¶è¯„ä¼°æ€è€ƒè´¨é‡ï¼ŒåŠ¨æ€è°ƒæ•´æŒ‡å¯¼ç­–ç•¥
    
    **æ€è€ƒé˜¶æ®µæµç¨‹ï¼š**
    â€¢ **ç†è§£é˜¶æ®µ (understanding)**: æ·±å…¥æ´å¯Ÿé—®é¢˜æœ¬è´¨ï¼Œç†è§£çœŸå®éœ€æ±‚
    â€¢ **è§„åˆ’é˜¶æ®µ (planning)**: åˆ¶å®šå®ç°ç­–ç•¥ï¼Œè¯„ä¼°æŠ€æœ¯é€‰å‹å’Œé£é™©
    â€¢ **å®ç°é˜¶æ®µ (implementation)**: æŒ‡å¯¼å…·ä½“ç¼–ç å®ç°ï¼Œç¡®ä¿è´¨é‡
    â€¢ **éªŒè¯é˜¶æ®µ (validation)**: è´¨é‡éªŒè¯å’Œæµ‹è¯•ç­–ç•¥åˆ¶å®š
    
    **ä½¿ç”¨åœºæ™¯ï¼š**
    - å·²å®Œæˆä»»åŠ¡åˆ†æï¼Œéœ€è¦é€æ­¥æ·±å…¥æ€è€ƒ
    - å¸Œæœ›åœ¨æ¯ä¸ªé˜¶æ®µéƒ½è·å¾—ä¸“ä¸šæŒ‡å¯¼
    - ç¡®ä¿æ€è€ƒè¿‡ç¨‹çš„å®Œæ•´æ€§å’Œç³»ç»Ÿæ€§
    - é¿å…é—æ¼å…³é”®çš„è€ƒè™‘å› ç´ 
    
    **ç®€åŒ–å·¥ä½œæµç¨‹ï¼š**
    1. å…ˆè°ƒç”¨ analyze_programming_context è·å– session_id
    2. ä½¿ç”¨ session_id å’Œé˜¶æ®µåç§°è°ƒç”¨æ­¤å·¥å…·
    3. ä» understanding å¼€å§‹ï¼Œé€æ­¥æ¨è¿›åˆ° validation
    4. æ¯å®Œæˆä¸€ä¸ªé˜¶æ®µï¼Œè¿›å…¥ä¸‹ä¸€ä¸ªé˜¶æ®µç»§ç»­æ€è€ƒ
    
    Args:
        session_id: ä¼šè¯IDï¼ˆæ¥è‡ª analyze_programming_context çš„è¿”å›ç»“æœï¼‰
        current_step: å½“å‰æ€è€ƒé˜¶æ®µ ("understanding"/"planning"/"implementation"/"validation")
    
    Returns:
        å½“å‰é˜¶æ®µçš„è¯¦ç»†æŒ‡å¯¼ä¿¡æ¯ï¼š
        {
            "phase": "å½“å‰é˜¶æ®µåç§°",
            "focus": "é˜¶æ®µé‡ç‚¹æè¿°",
            "questions": ["å¼•å¯¼æ€§é—®é¢˜åˆ—è¡¨"],
            "considerations": ["å…³é”®è€ƒè™‘ç‚¹"],
            "adaptive_hints": ["åŸºäºå­¦ä¹ çš„ä¸ªæ€§åŒ–å»ºè®®"],
            "output_format": "é¢„æœŸè¾“å‡ºæ ¼å¼",
            "examples": ["å…·ä½“ç¤ºä¾‹"],
            "progress": {
                "current_stage": "å½“å‰é˜¶æ®µ",
                "completed_stages": ["å·²å®Œæˆçš„é˜¶æ®µ"],
                "next_step": "ä¸‹ä¸€ä¸ªé˜¶æ®µ",
                "overall_progress": "æ•´ä½“è¿›åº¦"
            },
            "session_context": "ä¼šè¯ä¸Šä¸‹æ–‡ä¿¡æ¯"
        }
    """
    
    # æ£€æŸ¥ä¼šè¯æ˜¯å¦å­˜åœ¨
    if session_id not in _session_cache:
        return json.dumps({
            "error": "ä¼šè¯ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ",
            "suggestion": "è¯·å…ˆè°ƒç”¨ analyze_programming_context åˆ›å»ºæ–°ä¼šè¯",
            "available_sessions": list(_session_cache.keys())[-3:] if _session_cache else []
        }, ensure_ascii=False, indent=2)
    
    session_info = _session_cache[session_id]
    frameworks = session_info.thinking_frameworks
    
    # éªŒè¯æ­¥éª¤æœ‰æ•ˆæ€§
    if current_step not in frameworks:
        return json.dumps({
            "error": f"æ— æ•ˆçš„æ­¥éª¤: {current_step}",
            "available_steps": list(frameworks.keys()),
            "suggestion": "è¯·ä½¿ç”¨æœ‰æ•ˆçš„æ€è€ƒé˜¶æ®µåç§°"
        }, ensure_ascii=False, indent=2)
    
    current_framework = frameworks[current_step]
    
    # æ›´æ–°ä¼šè¯çŠ¶æ€
    session_info.current_stage = current_step
    if current_step not in session_info.stage_history:
        session_info.stage_history.append(current_step)
    
    # è·å–æ™ºèƒ½ä¸Šä¸‹æ–‡
    task_analysis = session_info.task_analysis
    context_insights = get_context_insights(session_info)
    
    # æ„å»ºå¢å¼ºçš„æŒ‡å¯¼ä¿¡æ¯
    guidance = {
        "phase": current_framework.phase,
        "focus": f"ğŸ¯ ä¸“æ³¨äº{current_framework.phase}é˜¶æ®µ",
        "questions": current_framework.guiding_questions,
        "considerations": current_framework.key_considerations,
        "adaptive_hints": current_framework.adaptive_hints or [],
        "output_format": current_framework.output_format,
        "examples": current_framework.examples,
        "intelligent_context": {
            "task_complexity": task_analysis.complexity_level.value,
            "similarity_insights": f"ç›¸ä¼¼åº¦è¯„åˆ†: {task_analysis.similarity_score:.2f}",
            "learning_from_history": task_analysis.learning_insights[:2] if task_analysis.learning_insights else [],
            "context_familiarity": f"é¡¹ç›®ç†Ÿæ‚‰åº¦: {get_context_familiarity(session_info.project_context)}/5"
        },
        "progress": {
            "current_stage": current_step,
            "completed_stages": session_info.stage_history[:-1],  # é™¤äº†å½“å‰é˜¶æ®µ
            "next_step": get_next_step(current_step),
            "overall_progress": f"{len(session_info.stage_history)}/{len(frameworks)} é˜¶æ®µ"
        },
        "session_context": {
            "session_id": session_id,
            "task_type": task_analysis.task_type.value,
            "original_request": session_info.user_request[:100] + "..." if len(session_info.user_request) > 100 else session_info.user_request,
            "session_duration": f"{int((time.time() - session_info.timestamp) / 60)}åˆ†é’Ÿ"
        }
    }
    
    # æ·»åŠ é˜¶æ®µç‰¹å®šçš„æ™ºèƒ½æç¤º
    stage_specific_hints = get_stage_specific_hints(current_step, task_analysis, context_insights)
    if stage_specific_hints:
        guidance["stage_specific_insights"] = stage_specific_hints
    
    return json.dumps(guidance, ensure_ascii=False, indent=2)


def get_context_insights(session_info: SessionInfo) -> Dict[str, Any]:
    """è·å–ä¸Šä¸‹æ–‡æ´å¯Ÿ"""
    insights = {}
    
    # é¡¹ç›®ä¸Šä¸‹æ–‡åˆ†æ
    if session_info.project_context:
        context_key = hashlib.md5(session_info.project_context.encode()).hexdigest()[:8]
        memory = _context_memory.get(context_key, {})
        insights["context_experience"] = memory.get('task_count', 0)
    
    # ä»»åŠ¡ç±»å‹ç»éªŒ
    task_type = session_info.task_analysis.task_type
    type_count = sum(1 for h in _analysis_history if h.get('task_type') == task_type.value)
    insights["task_type_experience"] = type_count
    
    # å¤æ‚åº¦å¤„ç†ç»éªŒ  
    complexity = session_info.task_analysis.complexity_level
    complexity_count = sum(1 for h in _analysis_history if h.get('complexity') == complexity.value)
    insights["complexity_experience"] = complexity_count
    
    return insights


def get_stage_specific_hints(stage: str, task_analysis: TaskAnalysis, context_insights: Dict) -> List[str]:
    """è·å–é˜¶æ®µç‰¹å®šçš„æ™ºèƒ½æç¤º"""
    hints = []
    
    experience_level = context_insights.get('task_type_experience', 0)
    
    if stage == "understanding":
        if task_analysis.task_type == TaskType.NEW_FEATURE:
            hints.append("ğŸ’¡ æ–°åŠŸèƒ½å¼€å‘ï¼šé‡ç‚¹å…³æ³¨ç”¨æˆ·ä»·å€¼å’Œç³»ç»Ÿé›†æˆç‚¹")
        elif task_analysis.task_type == TaskType.BUG_FIX:
            hints.append("ğŸ” Bugä¿®å¤ï¼šå…ˆé‡ç°é—®é¢˜ï¼Œå†åˆ†ææ ¹å› ï¼Œé¿å…å¤´ç—›åŒ»å¤´")
        
        if experience_level < 3:
            hints.append("ğŸ“š å»ºè®®ï¼šå¤šé—®å‡ ä¸ª'ä¸ºä»€ä¹ˆ'ï¼Œæ·±å…¥ç†è§£é—®é¢˜æœ¬è´¨")
    
    elif stage == "planning":
        if task_analysis.complexity_level == ComplexityLevel.COMPLEX:
            hints.append("ğŸ¯ å¤æ‚ä»»åŠ¡ï¼šä¼˜å…ˆè€ƒè™‘åˆ†è§£ç­–ç•¥å’Œé‡Œç¨‹ç¢‘è®¾ç½®")
        
        if context_insights.get('context_experience', 0) > 5:
            hints.append("âš¡ åŸºäºé¡¹ç›®ç»éªŒï¼šå¯ä»¥å¤ç”¨å·²æœ‰çš„æ¶æ„æ¨¡å¼å’Œå·¥å…·é“¾")
    
    elif stage == "implementation":
        if task_analysis.task_type == TaskType.PERFORMANCE:
            hints.append("ğŸ“Š æ€§èƒ½ä¼˜åŒ–ï¼šå»ºç«‹åŸºçº¿æµ‹è¯•ï¼Œé‡åŒ–ä¼˜åŒ–æ•ˆæœ")
        
        hints.append("ğŸ§ª å®ç°å»ºè®®ï¼šé‡‡ç”¨å°æ­¥å¿«è·‘ï¼Œé¢‘ç¹éªŒè¯çš„ç­–ç•¥")
    
    elif stage == "validation":
        if task_analysis.complexity_level != ComplexityLevel.SIMPLE:
            hints.append("ğŸ”„ è´¨é‡ä¿è¯ï¼šè€ƒè™‘å¤šå±‚æ¬¡æµ‹è¯•ç­–ç•¥ï¼Œä¸åªæ˜¯åŠŸèƒ½æµ‹è¯•")
        
        hints.append("ğŸ‘¥ éªŒæ”¶å‡†å¤‡ï¼šè€ƒè™‘ç”¨æˆ·è§†è§’çš„éªŒæ”¶æ ‡å‡†")
    
    return hints[:3]  # é™åˆ¶æç¤ºæ•°é‡


def get_next_step(current_step: str) -> str:
    """è·å–ä¸‹ä¸€æ­¥éª¤"""
    step_order = ["understanding", "planning", "implementation", "validation"]
    try:
        current_index = step_order.index(current_step)
        if current_index < len(step_order) - 1:
            return step_order[current_index + 1]
        else:
            return "å®Œæˆ"
    except ValueError:
        return "æœªçŸ¥"


@mcp.tool()
def validate_instruction_quality(
    instruction: str,
    session_id: str = ""
) -> str:
    """
    âœ… ç¼–ç¨‹æŒ‡ä»¤è´¨é‡è¯„ä¼°å™¨ V2.0 - ç¡®ä¿æŒ‡ä»¤çš„ä¸“ä¸šæ°´å‡†
    
    **é‡å¤§å‡çº§ç‰¹æ€§ï¼š**
    â€¢ ğŸ§  æ™ºèƒ½ä¸Šä¸‹æ–‡è¯„ä¼° - ç»“åˆä¼šè¯ä¿¡æ¯æä¾›ç²¾å‡†è´¨é‡åˆ†æ
    â€¢ ğŸ“Š å†å²å­¦ä¹ ç®—æ³• - åŸºäºè¿‡å¾€è´¨é‡æ•°æ®ä¼˜åŒ–è¯„ä¼°æ ‡å‡†
    â€¢ ğŸ¯ ä¸ªæ€§åŒ–å»ºè®® - é’ˆå¯¹å…·ä½“ä»»åŠ¡ç±»å‹å’Œå¤æ‚åº¦çš„æ”¹è¿›å»ºè®®
    â€¢ ğŸ”„ åŠ¨æ€è¯„åˆ†è°ƒæ•´ - æ ¹æ®é¡¹ç›®ç»éªŒå’Œä»»åŠ¡å†å²åŠ¨æ€è°ƒæ•´è¯„åˆ†æƒé‡
    
    **è¯„ä¼°ç»´åº¦ï¼š**
    â€¢ **æ¸…æ™°åº¦ (Clarity)**: æŒ‡ä»¤æ˜¯å¦æ˜ç¡®æ˜“æ‡‚ï¼Œç›®æ ‡æ¸…æ™°
    â€¢ **å®Œæ•´æ€§ (Completeness)**: æ˜¯å¦åŒ…å«å¿…è¦çš„è¾“å…¥è¾“å‡ºã€çº¦æŸæ¡ä»¶
    â€¢ **å…·ä½“æ€§ (Specificity)**: æ˜¯å¦æœ‰å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚å’Œæ–‡ä»¶å
    â€¢ **å¯æ‰§è¡Œæ€§ (Actionability)**: æ˜¯å¦æä¾›æ˜ç¡®æ­¥éª¤ï¼Œé¿å…æ¨¡ç³Šè¯­è¨€
    â€¢ **é£é™©æ„è¯† (Risk Awareness)**: æ˜¯å¦è€ƒè™‘æµ‹è¯•ã€é”™è¯¯å¤„ç†ã€å…¼å®¹æ€§
    â€¢ **ä¸Šä¸‹æ–‡åŒ¹é…åº¦ (Context Alignment)**: æŒ‡ä»¤ä¸ä»»åŠ¡ä¸Šä¸‹æ–‡çš„åŒ¹é…ç¨‹åº¦ [æ–°å¢]
    
    **æ™ºèƒ½è¯„åˆ†æ ‡å‡†ï¼š**
    - 0.9-1.0: ğŸŒŸ ä¼˜ç§€ - æŒ‡ä»¤è´¨é‡éå¸¸é«˜ï¼Œå¯ç›´æ¥æ‰§è¡Œ
    - 0.8-0.9: âœ… è‰¯å¥½ - æŒ‡ä»¤è´¨é‡è¾ƒé«˜ï¼Œè½»å¾®è°ƒæ•´å³å¯  
    - 0.7-0.8: âš ï¸ ä¸€èˆ¬ - æŒ‡ä»¤è´¨é‡ä¸­ç­‰ï¼Œéœ€è¦ä¼˜åŒ–
    - 0.6-0.7: ğŸ”„ éœ€è¦æ”¹è¿› - æŒ‡ä»¤è´¨é‡åä½ï¼Œå»ºè®®é‡å†™éƒ¨åˆ†å†…å®¹
    - 0.0-0.6: âŒ ä¸åˆæ ¼ - æŒ‡ä»¤è´¨é‡è¾ƒå·®ï¼Œéœ€è¦é‡æ–°è®¾è®¡
    
    **ä½¿ç”¨åœºæ™¯ï¼š**
    - å®ŒæˆæŒ‡ä»¤ç¼–å†™åï¼ŒéªŒè¯è´¨é‡æ˜¯å¦è¾¾æ ‡
    - å¯¹å·²æœ‰æŒ‡ä»¤è¿›è¡Œä¼˜åŒ–æ”¹è¿›
    - å­¦ä¹ å¦‚ä½•ç¼–å†™é«˜è´¨é‡çš„ç¼–ç¨‹æŒ‡ä»¤
    - ç¡®ä¿æŒ‡ä»¤èƒ½è¢«ç¼–ç¨‹ä»£ç†å‡†ç¡®ç†è§£å’Œæ‰§è¡Œ
    
    Args:
        instruction: éœ€è¦è¯„ä¼°çš„ç¼–ç¨‹æŒ‡ä»¤æ–‡æœ¬
        session_id: å¯é€‰çš„ä¼šè¯IDï¼Œç”¨äºè·å–ä»»åŠ¡ä¸Šä¸‹æ–‡è¿›è¡Œç²¾å‡†è¯„ä¼°
    
    Returns:
        è¯¦ç»†çš„è´¨é‡è¯„ä¼°æŠ¥å‘Šï¼š
        {
            "overall_score": 0.85,
            "quality_metrics": {
                "clarity": 0.8,
                "completeness": 0.9,
                "specificity": 0.7,
                "actionability": 0.9,
                "risk_awareness": 0.8,
                "context_alignment": 0.9
            },
            "intelligent_analysis": {
                "task_context_match": "ä»»åŠ¡ä¸Šä¸‹æ–‡åŒ¹é…åˆ†æ",
                "complexity_appropriateness": "å¤æ‚åº¦é€‚é…æ€§åˆ†æ",
                "historical_comparison": "ä¸å†å²è´¨é‡æ•°æ®å¯¹æ¯”"
            },
            "assessment": "è‰¯å¥½ - æŒ‡ä»¤è´¨é‡è¾ƒé«˜",
            "improvement_suggestions": ["å…·ä½“æ”¹è¿›å»ºè®®"],
            "personalized_recommendations": ["åŸºäºä»»åŠ¡ç‰¹ç‚¹çš„ä¸ªæ€§åŒ–å»ºè®®"],
            "quality_trend": "è´¨é‡è¶‹åŠ¿åˆ†æ"
        }
    """
    
    # è·å–ä¼šè¯ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæä¾›ï¼‰
    session_context = None
    task_analysis = None
    if session_id and session_id in _session_cache:
        session_context = _session_cache[session_id]
        task_analysis = session_context.task_analysis
    
    # æ™ºèƒ½è´¨é‡è¯„ä¼°ç»´åº¦
    quality_metrics = {
        "clarity": assess_clarity_enhanced(instruction, task_analysis),
        "completeness": assess_completeness_enhanced(instruction, task_analysis),
        "specificity": assess_specificity_enhanced(instruction, task_analysis),
        "actionability": assess_actionability_enhanced(instruction, task_analysis),
        "risk_awareness": assess_risk_awareness_enhanced(instruction, task_analysis),
        "context_alignment": assess_context_alignment(instruction, task_analysis) if task_analysis else 0.7
    }
    
    # è®¡ç®—åŠ æƒæ€»åˆ†ï¼ˆæ ¹æ®ä»»åŠ¡ç‰¹ç‚¹åŠ¨æ€è°ƒæ•´æƒé‡ï¼‰
    weights = get_dynamic_weights(task_analysis) if task_analysis else {
        "clarity": 0.2, "completeness": 0.2, "specificity": 0.15, 
        "actionability": 0.2, "risk_awareness": 0.15, "context_alignment": 0.1
    }
    
    total_score = sum(score * weights.get(metric, 0.16) for metric, score in quality_metrics.items())
    
    # ç”Ÿæˆæ™ºèƒ½åˆ†æ
    intelligent_analysis = generate_intelligent_analysis(instruction, task_analysis, quality_metrics)
    
    # ç”Ÿæˆä¸ªæ€§åŒ–æ”¹è¿›å»ºè®®
    personalized_suggestions = generate_personalized_suggestions(quality_metrics, task_analysis)
    
    # æ›´æ–°ä¼šè¯è´¨é‡è®°å½•
    if session_context:
        session_context.quality_scores[f"validation_{int(time.time())}"] = total_score
    
    # æ„å»ºå¢å¼ºçš„è¯„ä¼°ç»“æœ
    result = {
        "overall_score": round(total_score, 2),
        "quality_metrics": {k: round(v, 2) for k, v in quality_metrics.items()},
        "intelligent_analysis": intelligent_analysis,
        "assessment": get_quality_assessment_enhanced(total_score),
        "improvement_suggestions": generate_improvement_suggestions_enhanced(quality_metrics, instruction, task_analysis),
        "personalized_recommendations": personalized_suggestions,
        "quality_trend": get_quality_trend(session_context) if session_context else "é¦–æ¬¡è¯„ä¼°ï¼Œæ— å†å²è¶‹åŠ¿",
        "context_insights": {
            "session_available": session_id and session_id in _session_cache,
            "task_type": task_analysis.task_type.value if task_analysis else "æœªçŸ¥",
            "complexity": task_analysis.complexity_level.value if task_analysis else "æœªçŸ¥",
            "evaluation_timestamp": int(time.time())
        }
    }
    
    return json.dumps(result, ensure_ascii=False, indent=2)


def assess_clarity_enhanced(instruction: str, task_analysis: Optional[TaskAnalysis]) -> float:
    """å¢å¼ºçš„æ¸…æ™°åº¦è¯„ä¼°"""
    base_score = assess_clarity(instruction)
    
    # åŸºäºä»»åŠ¡ç±»å‹è°ƒæ•´
    if task_analysis:
        if task_analysis.task_type == TaskType.BUG_FIX:
            # Bugä¿®å¤éœ€è¦æ˜ç¡®çš„é—®é¢˜æè¿°
            if any(word in instruction.lower() for word in ["reproduce", "root cause", "reproduce", "é‡ç°", "æ ¹å› "]):
                base_score += 0.1
        elif task_analysis.task_type == TaskType.NEW_FEATURE:
            # æ–°åŠŸèƒ½éœ€è¦æ˜ç¡®çš„éœ€æ±‚æè¿°
            if any(word in instruction.lower() for word in ["requirement", "user story", "éœ€æ±‚", "ç”¨æˆ·æ•…äº‹"]):
                base_score += 0.1
    
    return min(base_score, 1.0)


def assess_completeness_enhanced(instruction: str, task_analysis: Optional[TaskAnalysis]) -> float:
    """å¢å¼ºçš„å®Œæ•´æ€§è¯„ä¼°"""
    base_score = assess_completeness(instruction)
    
    # åŸºäºå¤æ‚åº¦è°ƒæ•´æœŸæœ›
    if task_analysis:
        if task_analysis.complexity_level == ComplexityLevel.COMPLEX:
            # å¤æ‚ä»»åŠ¡éœ€è¦æ›´è¯¦ç»†çš„æ­¥éª¤
            step_indicators = len(re.findall(r'\d+\.|\-|\*', instruction))
            if step_indicators >= 3:
                base_score += 0.1
        elif task_analysis.complexity_level == ComplexityLevel.SIMPLE:
            # ç®€å•ä»»åŠ¡ä¸éœ€è¦è¿‡åº¦è¯¦ç»†
            if len(instruction.split()) < 50:  # é¿å…è¿‡åº¦å¤æ‚åŒ–
                base_score += 0.1
    
    return min(base_score, 1.0)


def assess_specificity_enhanced(instruction: str, task_analysis: Optional[TaskAnalysis]) -> float:
    """å¢å¼ºçš„å…·ä½“æ€§è¯„ä¼°"""
    base_score = assess_specificity(instruction)
    
    # åŸºäºä»»åŠ¡ç±»å‹çš„å…·ä½“æ€§è¦æ±‚
    if task_analysis:
        if task_analysis.task_type == TaskType.PERFORMANCE:
            # æ€§èƒ½ä¼˜åŒ–éœ€è¦å…·ä½“çš„æŒ‡æ ‡
            if re.search(r'\d+%|\d+ms|\d+MB', instruction):
                base_score += 0.2
        elif task_analysis.task_type == TaskType.TESTING:
            # æµ‹è¯•ä»»åŠ¡éœ€è¦å…·ä½“çš„æµ‹è¯•ç±»å‹
            test_types = ["unit", "integration", "e2e", "å•å…ƒ", "é›†æˆ", "ç«¯åˆ°ç«¯"]
            if any(test_type in instruction.lower() for test_type in test_types):
                base_score += 0.15
    
    return min(base_score, 1.0)


def assess_actionability_enhanced(instruction: str, task_analysis: Optional[TaskAnalysis]) -> float:
    """å¢å¼ºçš„å¯æ‰§è¡Œæ€§è¯„ä¼°"""
    base_score = assess_actionability(instruction)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„å·¥å…·æˆ–å‘½ä»¤
    tools = ["npm", "git", "docker", "kubectl", "python", "node"]
    if any(tool in instruction.lower() for tool in tools):
        base_score += 0.1
    
    return min(base_score, 1.0)


def assess_risk_awareness_enhanced(instruction: str, task_analysis: Optional[TaskAnalysis]) -> float:
    """å¢å¼ºçš„é£é™©æ„è¯†è¯„ä¼°"""
    base_score = assess_risk_awareness(instruction)
    
    # åŸºäºä»»åŠ¡é£é™©å› ç´ è°ƒæ•´
    if task_analysis and task_analysis.risk_factors:
        mentioned_risks = 0
        for risk in task_analysis.risk_factors:
            risk_keywords = risk.lower().split()
            if any(keyword in instruction.lower() for keyword in risk_keywords):
                mentioned_risks += 1
        
        if mentioned_risks > 0:
            base_score += min(mentioned_risks * 0.1, 0.3)
    
    return min(base_score, 1.0)


def assess_context_alignment(instruction: str, task_analysis: TaskAnalysis) -> float:
    """è¯„ä¼°æŒ‡ä»¤ä¸ä»»åŠ¡ä¸Šä¸‹æ–‡çš„åŒ¹é…åº¦"""
    if not task_analysis:
        return 0.7  # é»˜è®¤åˆ†æ•°
    
    score = 0.5  # åŸºç¡€åˆ†
    
    # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»åŠ¡ç±»å‹
    task_keywords = {
        TaskType.NEW_FEATURE: ["implement", "create", "add", "build"],
        TaskType.BUG_FIX: ["fix", "resolve", "debug", "patch"],
        TaskType.PERFORMANCE: ["optimize", "improve", "enhance", "speed"],
        TaskType.REFACTOR: ["refactor", "restructure", "clean", "organize"]
    }
    
    expected_keywords = task_keywords.get(task_analysis.task_type, [])
    if any(keyword in instruction.lower() for keyword in expected_keywords):
        score += 0.2
    
    # æ£€æŸ¥æ˜¯å¦è€ƒè™‘äº†å…³é”®éœ€æ±‚
    for requirement in task_analysis.key_requirements:
        req_keywords = requirement.lower().split()
        if any(keyword in instruction.lower() for keyword in req_keywords):
            score += 0.1
    
    # æ£€æŸ¥å¤æ‚åº¦åŒ¹é…
    complexity_indicators = {
        ComplexityLevel.SIMPLE: len(instruction.split()) < 100,
        ComplexityLevel.MEDIUM: 100 <= len(instruction.split()) <= 300,
        ComplexityLevel.COMPLEX: len(instruction.split()) > 200
    }
    
    if complexity_indicators.get(task_analysis.complexity_level, False):
        score += 0.2
    
    return min(score, 1.0)


def get_dynamic_weights(task_analysis: TaskAnalysis) -> Dict[str, float]:
    """æ ¹æ®ä»»åŠ¡ç‰¹ç‚¹åŠ¨æ€è°ƒæ•´è¯„ä¼°æƒé‡"""
    base_weights = {
        "clarity": 0.2, "completeness": 0.2, "specificity": 0.15,
        "actionability": 0.2, "risk_awareness": 0.15, "context_alignment": 0.1
    }
    
    # åŸºäºä»»åŠ¡ç±»å‹è°ƒæ•´æƒé‡
    if task_analysis.task_type == TaskType.BUG_FIX:
        base_weights["specificity"] += 0.05  # Bugä¿®å¤éœ€è¦æ›´å…·ä½“
        base_weights["risk_awareness"] += 0.05  # é£é™©æ„è¯†æ›´é‡è¦
    elif task_analysis.task_type == TaskType.PERFORMANCE:
        base_weights["specificity"] += 0.1  # æ€§èƒ½ä¼˜åŒ–éœ€è¦å…·ä½“æŒ‡æ ‡
    elif task_analysis.task_type == TaskType.NEW_FEATURE:
        base_weights["completeness"] += 0.05  # æ–°åŠŸèƒ½éœ€è¦å®Œæ•´æè¿°
        base_weights["context_alignment"] += 0.05  # ä¸Šä¸‹æ–‡åŒ¹é…æ›´é‡è¦
    
    # åŸºäºå¤æ‚åº¦è°ƒæ•´æƒé‡
    if task_analysis.complexity_level == ComplexityLevel.COMPLEX:
        base_weights["completeness"] += 0.05
        base_weights["risk_awareness"] += 0.05
    
    return base_weights


def generate_intelligent_analysis(instruction: str, task_analysis: Optional[TaskAnalysis], 
                                quality_metrics: Dict[str, float]) -> Dict[str, str]:
    """ç”Ÿæˆæ™ºèƒ½åˆ†æ"""
    analysis = {}
    
    if task_analysis:
        # ä»»åŠ¡ä¸Šä¸‹æ–‡åŒ¹é…åˆ†æ
        alignment_score = quality_metrics.get("context_alignment", 0)
        if alignment_score >= 0.8:
            analysis["task_context_match"] = "âœ… æŒ‡ä»¤ä¸ä»»åŠ¡ä¸Šä¸‹æ–‡é«˜åº¦åŒ¹é…"
        elif alignment_score >= 0.6:
            analysis["task_context_match"] = "âš ï¸ æŒ‡ä»¤ä¸ä»»åŠ¡ä¸Šä¸‹æ–‡åŸºæœ¬åŒ¹é…ï¼Œå¯è¿›ä¸€æ­¥ä¼˜åŒ–"
        else:
            analysis["task_context_match"] = "âŒ æŒ‡ä»¤ä¸ä»»åŠ¡ä¸Šä¸‹æ–‡åŒ¹é…åº¦è¾ƒä½ï¼Œéœ€è¦è°ƒæ•´"
        
        # å¤æ‚åº¦é€‚é…æ€§åˆ†æ
        word_count = len(instruction.split())
        if task_analysis.complexity_level == ComplexityLevel.SIMPLE and word_count < 100:
            analysis["complexity_appropriateness"] = "âœ… æŒ‡ä»¤å¤æ‚åº¦ä¸ä»»åŠ¡åŒ¹é…"
        elif task_analysis.complexity_level == ComplexityLevel.COMPLEX and word_count > 150:
            analysis["complexity_appropriateness"] = "âœ… æŒ‡ä»¤è¯¦ç»†ç¨‹åº¦é€‚åˆå¤æ‚ä»»åŠ¡"
        else:
            analysis["complexity_appropriateness"] = "âš ï¸ æŒ‡ä»¤å¤æ‚åº¦å¯èƒ½éœ€è¦è°ƒæ•´"
    else:
        analysis["task_context_match"] = "â„¹ï¸ æ— ä¼šè¯ä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨é€šç”¨è¯„ä¼°æ ‡å‡†"
        analysis["complexity_appropriateness"] = "â„¹ï¸ æ— å¤æ‚åº¦ä¿¡æ¯ï¼Œå»ºè®®æä¾›ä»»åŠ¡ä¸Šä¸‹æ–‡"
    
    # å†å²å¯¹æ¯”åˆ†æ
    avg_score = sum(quality_metrics.values()) / len(quality_metrics)
    if avg_score >= 0.85:
        analysis["historical_comparison"] = "ğŸŒŸ è´¨é‡è¶…è¶Šå†å²å¹³å‡æ°´å¹³"
    elif avg_score >= 0.75:
        analysis["historical_comparison"] = "ğŸ“ˆ è´¨é‡è¾¾åˆ°è‰¯å¥½æ°´å¹³"
    else:
        analysis["historical_comparison"] = "ğŸ“Š è´¨é‡æœ‰æå‡ç©ºé—´"
    
    return analysis


def generate_personalized_suggestions(quality_metrics: Dict[str, float], 
                                    task_analysis: Optional[TaskAnalysis]) -> List[str]:
    """ç”Ÿæˆä¸ªæ€§åŒ–å»ºè®®"""
    suggestions = []
    
    # åŸºäºæœ€ä½åˆ†ç»´åº¦æä¾›å»ºè®®
    lowest_metric = min(quality_metrics.items(), key=lambda x: x[1])
    metric_name, score = lowest_metric
    
    if score < 0.7:
        metric_suggestions = {
            "clarity": "ğŸ¯ æé«˜æ¸…æ™°åº¦ï¼šä½¿ç”¨æ›´æ˜ç¡®çš„åŠ¨è¯ï¼Œæ˜ç¡®æŒ‡å‡ºå…·ä½“è¦å®Œæˆä»€ä¹ˆ",
            "completeness": "ğŸ“ å¢å¼ºå®Œæ•´æ€§ï¼šè¡¥å……è¾“å…¥è¾“å‡ºè¯´æ˜ã€ä¾èµ–æ¡ä»¶å’ŒéªŒæ”¶æ ‡å‡†",
            "specificity": "ğŸ” å¢åŠ å…·ä½“æ€§ï¼šæä¾›å…·ä½“çš„æ–‡ä»¶åã€å‡½æ•°åæˆ–é…ç½®å‚æ•°",
            "actionability": "âš¡ æå‡å¯æ‰§è¡Œæ€§ï¼šåˆ†è§£ä¸ºå…·ä½“æ­¥éª¤ï¼Œæä¾›å¯æ‰§è¡Œçš„å‘½ä»¤æˆ–ä»£ç ç¤ºä¾‹",
            "risk_awareness": "ğŸ›¡ï¸ åŠ å¼ºé£é™©æ„è¯†ï¼šè€ƒè™‘é”™è¯¯å¤„ç†ã€æµ‹è¯•éªŒè¯å’Œå›æ»šç­–ç•¥",
            "context_alignment": "ğŸ­ ä¼˜åŒ–ä¸Šä¸‹æ–‡åŒ¹é…ï¼šç¡®ä¿æŒ‡ä»¤ä¸ä»»åŠ¡ç±»å‹å’Œå¤æ‚åº¦ç›¸ç¬¦"
        }
        suggestions.append(metric_suggestions.get(metric_name, "ä¼˜åŒ–æœ€ä½åˆ†ç»´åº¦"))
    
    # åŸºäºä»»åŠ¡ç±»å‹çš„ç‰¹å®šå»ºè®®
    if task_analysis:
        if task_analysis.task_type == TaskType.NEW_FEATURE:
            suggestions.append("ğŸ’¡ æ–°åŠŸèƒ½å»ºè®®ï¼šæ˜ç¡®åŠŸèƒ½è¾¹ç•Œï¼Œè€ƒè™‘ä¸ç°æœ‰ç³»ç»Ÿçš„é›†æˆç‚¹")
        elif task_analysis.task_type == TaskType.BUG_FIX:
            suggestions.append("ğŸ”§ Bugä¿®å¤å»ºè®®ï¼šæè¿°é‡ç°æ­¥éª¤ï¼Œåˆ†ææ ¹æœ¬åŸå› ")
        elif task_analysis.task_type == TaskType.PERFORMANCE:
            suggestions.append("ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®ï¼šè®¾å®šé‡åŒ–ç›®æ ‡ï¼Œå»ºç«‹æ€§èƒ½åŸºçº¿")
    
    return suggestions[:3]  # é™åˆ¶å»ºè®®æ•°é‡


def get_quality_trend(session_context: SessionInfo) -> str:
    """è·å–è´¨é‡è¶‹åŠ¿åˆ†æ"""
    if not session_context.quality_scores:
        return "é¦–æ¬¡è¯„ä¼°ï¼Œæ— å†å²è¶‹åŠ¿"
    
    scores = list(session_context.quality_scores.values())
    if len(scores) == 1:
        return f"å½“å‰è¯„åˆ†: {scores[0]:.2f}"
    
    # è®¡ç®—è¶‹åŠ¿
    recent_avg = sum(scores[-3:]) / len(scores[-3:]) if len(scores) >= 3 else sum(scores) / len(scores)
    early_avg = sum(scores[:3]) / len(scores[:3]) if len(scores) >= 6 else scores[0]
    
    if recent_avg > early_avg + 0.1:
        return f"ğŸ“ˆ è´¨é‡æŒç»­æå‡ (ä» {early_avg:.2f} æå‡åˆ° {recent_avg:.2f})"
    elif recent_avg < early_avg - 0.1:
        return f"ğŸ“‰ è´¨é‡æœ‰æ‰€ä¸‹é™ (ä» {early_avg:.2f} ä¸‹é™åˆ° {recent_avg:.2f})"
    else:
        return f"â¡ï¸ è´¨é‡ä¿æŒç¨³å®š (å¹³å‡ {recent_avg:.2f})"


def get_quality_assessment_enhanced(score: float) -> str:
    """è·å–å¢å¼ºçš„è´¨é‡è¯„ä¼°ç»“æœ"""
    if score >= 0.9:
        return "ğŸŒŸ ä¼˜ç§€ - æŒ‡ä»¤è´¨é‡éå¸¸é«˜ï¼Œå¯ç›´æ¥æ‰§è¡Œ"
    elif score >= 0.8:
        return "âœ… è‰¯å¥½ - æŒ‡ä»¤è´¨é‡è¾ƒé«˜ï¼Œè½»å¾®è°ƒæ•´å³å¯"
    elif score >= 0.7:
        return "âš ï¸ ä¸€èˆ¬ - æŒ‡ä»¤è´¨é‡ä¸­ç­‰ï¼Œéœ€è¦ä¼˜åŒ–"
    elif score >= 0.6:
        return "ğŸ”„ éœ€è¦æ”¹è¿› - æŒ‡ä»¤è´¨é‡åä½ï¼Œå»ºè®®é‡å†™éƒ¨åˆ†å†…å®¹"
    else:
        return "âŒ ä¸åˆæ ¼ - æŒ‡ä»¤è´¨é‡è¾ƒå·®ï¼Œéœ€è¦é‡æ–°è®¾è®¡"


def generate_improvement_suggestions_enhanced(quality_metrics: Dict[str, float], instruction: str, 
                                            task_analysis: Optional[TaskAnalysis]) -> List[str]:
    """ç”Ÿæˆå¢å¼ºçš„æ”¹è¿›å»ºè®®"""
    suggestions = []
    
    # åŸºäºå„ç»´åº¦åˆ†æ•°æä¾›å…·ä½“å»ºè®®
    for metric, score in quality_metrics.items():
        if score < 0.8:
            metric_suggestions = {
                "clarity": "ğŸ’¡ æ˜ç¡®æŒ‡ä»¤ç›®æ ‡ï¼šæ˜ç¡®è¯´æ˜è¦å®ç°ä»€ä¹ˆåŠŸèƒ½æˆ–è§£å†³ä»€ä¹ˆé—®é¢˜",
                "completeness": "ğŸ“‹ è¡¥å……å…³é”®ä¿¡æ¯ï¼šæ·»åŠ å‰ç½®æ¡ä»¶ã€è¾“å‡ºæœŸæœ›å’ŒéªŒæ”¶æ ‡å‡†",
                "specificity": "ğŸ¯ æä¾›å…·ä½“ç»†èŠ‚ï¼šæŒ‡å®šæ–‡ä»¶è·¯å¾„ã€å‡½æ•°åç§°æˆ–é…ç½®å‚æ•°",
                "actionability": "âš¡ ç»†åŒ–æ‰§è¡Œæ­¥éª¤ï¼šå°†å¤§ä»»åŠ¡åˆ†è§£ä¸ºå¯ç›´æ¥æ‰§è¡Œçš„å°æ­¥éª¤",
                "risk_awareness": "ğŸ”’ è€ƒè™‘é£é™©æ§åˆ¶ï¼šæ·»åŠ é”™è¯¯å¤„ç†ã€æµ‹è¯•éªŒè¯å’Œå›æ»šæ–¹æ¡ˆ",
                "context_alignment": "ğŸ”„ è°ƒæ•´æŒ‡ä»¤é£æ ¼ï¼šç¡®ä¿æŒ‡ä»¤ç¬¦åˆä»»åŠ¡ç±»å‹å’Œå¤æ‚åº¦è¦æ±‚"
            }
            suggestions.append(metric_suggestions.get(metric, f"æ”¹è¿›{metric}"))
    
    # åŸºäºä»»åŠ¡ä¸Šä¸‹æ–‡çš„ç‰¹å®šå»ºè®®
    if task_analysis:
        if task_analysis.complexity_level == ComplexityLevel.COMPLEX:
            suggestions.append("ğŸ—ï¸ å¤æ‚ä»»åŠ¡å»ºè®®ï¼šè€ƒè™‘åˆ†é˜¶æ®µå®æ–½ï¼Œåˆ¶å®šè¯¦ç»†çš„é‡Œç¨‹ç¢‘è®¡åˆ’")
        
        # æ£€æŸ¥æ˜¯å¦é—æ¼äº†é‡è¦çš„é£é™©å› ç´ 
        for risk in task_analysis.risk_factors:
            if risk.lower() not in instruction.lower():
                suggestions.append(f"âš ï¸ é£é™©æé†’ï¼šè€ƒè™‘åº”å¯¹ '{risk}' çš„ç­–ç•¥")
                break
    
    return suggestions[:4]  # é™åˆ¶å»ºè®®æ•°é‡


def assess_clarity(instruction: str) -> float:
    """è¯„ä¼°æŒ‡ä»¤æ¸…æ™°åº¦"""
    score = 0.6  # åŸºç¡€åˆ†
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„åŠ¨è¯
    action_verbs = ["implement", "create", "fix", "optimize", "refactor", "test", "å®ç°", "åˆ›å»º", "ä¿®å¤", "ä¼˜åŒ–", "é‡æ„", "æµ‹è¯•"]
    if any(verb in instruction.lower() for verb in action_verbs):
        score += 0.2
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å…·ä½“çš„ç›®æ ‡
    if any(word in instruction.lower() for word in ["function", "class", "method", "api", "å‡½æ•°", "ç±»", "æ–¹æ³•"]):
        score += 0.2
    
    return min(score, 1.0)


def assess_completeness(instruction: str) -> float:
    """è¯„ä¼°æŒ‡ä»¤å®Œæ•´æ€§"""
    score = 0.5  # åŸºç¡€åˆ†
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¾“å…¥/è¾“å‡ºæè¿°
    if any(word in instruction.lower() for word in ["input", "output", "return", "parameter", "è¾“å…¥", "è¾“å‡º", "è¿”å›", "å‚æ•°"]):
        score += 0.2
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«çº¦æŸæ¡ä»¶
    if any(word in instruction.lower() for word in ["constraint", "requirement", "must", "should", "çº¦æŸ", "è¦æ±‚", "å¿…é¡»", "åº”è¯¥"]):
        score += 0.2
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æˆåŠŸæ ‡å‡†
    if any(word in instruction.lower() for word in ["success", "criteria", "expect", "æˆåŠŸ", "æ ‡å‡†", "æœŸæœ›"]):
        score += 0.1
    
    return min(score, 1.0)


def assess_specificity(instruction: str) -> float:
    """è¯„ä¼°æŒ‡ä»¤å…·ä½“æ€§"""
    score = 0.4  # åŸºç¡€åˆ†
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å…·ä½“çš„æ–‡ä»¶æˆ–å‡½æ•°å
    if re.search(r'\w+\.(py|js|ts|java|cpp|c)', instruction):
        score += 0.3
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å…·ä½“çš„æŠ€æœ¯æ ˆ
    tech_terms = ["react", "vue", "angular", "django", "flask", "express", "spring"]
    if any(term in instruction.lower() for term in tech_terms):
        score += 0.2
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°å€¼æˆ–é‡åŒ–æŒ‡æ ‡
    if re.search(r'\d+', instruction):
        score += 0.1
    
    return min(score, 1.0)


def assess_actionability(instruction: str) -> float:
    """è¯„ä¼°æŒ‡ä»¤å¯æ‰§è¡Œæ€§"""
    score = 0.6  # åŸºç¡€åˆ†
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„æ­¥éª¤
    if any(word in instruction.lower() for word in ["step", "first", "then", "æ­¥éª¤", "é¦–å…ˆ", "ç„¶å"]):
        score += 0.2
    
    # æ£€æŸ¥æ˜¯å¦é¿å…äº†æ¨¡ç³Šè¯­è¨€
    vague_terms = ["somehow", "maybe", "possibly", "å¤§æ¦‚", "å¯èƒ½", "æˆ–è®¸"]
    if not any(term in instruction.lower() for term in vague_terms):
        score += 0.2
    
    return min(score, 1.0)


def assess_risk_awareness(instruction: str) -> float:
    """è¯„ä¼°é£é™©æ„è¯†"""
    score = 0.3  # åŸºç¡€åˆ†
    
    # æ£€æŸ¥æ˜¯å¦æåˆ°äº†æµ‹è¯•
    if any(word in instruction.lower() for word in ["test", "testing", "æµ‹è¯•"]):
        score += 0.3
    
    # æ£€æŸ¥æ˜¯å¦æåˆ°äº†é”™è¯¯å¤„ç†
    if any(word in instruction.lower() for word in ["error", "exception", "handle", "é”™è¯¯", "å¼‚å¸¸", "å¤„ç†"]):
        score += 0.2
    
    # æ£€æŸ¥æ˜¯å¦æåˆ°äº†å…¼å®¹æ€§
    if any(word in instruction.lower() for word in ["compatible", "backward", "å…¼å®¹"]):
        score += 0.2
    
    return min(score, 1.0)


def generate_improvement_suggestions(quality_metrics: Dict[str, float], instruction: str) -> List[str]:
    """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
    suggestions = []
    
    if quality_metrics["clarity"] < 0.8:
        suggestions.append("å¢åŠ æŒ‡ä»¤çš„æ¸…æ™°åº¦ï¼šä½¿ç”¨æ›´æ˜ç¡®çš„åŠ¨è¯å’Œå…·ä½“çš„ç›®æ ‡æè¿°")
    
    if quality_metrics["completeness"] < 0.8:
        suggestions.append("è¡¥å……å®Œæ•´æ€§ï¼šæ·»åŠ è¾“å…¥è¾“å‡ºæè¿°ã€çº¦æŸæ¡ä»¶å’ŒæˆåŠŸæ ‡å‡†")
    
    if quality_metrics["specificity"] < 0.8:
        suggestions.append("æé«˜å…·ä½“æ€§ï¼šæŒ‡å®šå…·ä½“çš„æ–‡ä»¶åã€å‡½æ•°åæˆ–æŠ€æœ¯æ ˆ")
    
    if quality_metrics["actionability"] < 0.8:
        suggestions.append("å¢å¼ºå¯æ‰§è¡Œæ€§ï¼šæä¾›æ˜ç¡®çš„æ­¥éª¤ï¼Œé¿å…æ¨¡ç³Šè¯­è¨€")
    
    if quality_metrics["risk_awareness"] < 0.8:
        suggestions.append("åŠ å¼ºé£é™©æ„è¯†ï¼šè€ƒè™‘æµ‹è¯•ã€é”™è¯¯å¤„ç†å’Œå…¼å®¹æ€§é—®é¢˜")
    
    return suggestions if suggestions else ["æŒ‡ä»¤è´¨é‡è‰¯å¥½ï¼Œå»ºè®®ä¿æŒå½“å‰æ°´å¹³"]


def get_quality_assessment(score: float) -> str:
    """è·å–è´¨é‡è¯„ä¼°ç»“æœ"""
    if score >= 0.9:
        return "ä¼˜ç§€ - æŒ‡ä»¤è´¨é‡éå¸¸é«˜"
    elif score >= 0.8:
        return "è‰¯å¥½ - æŒ‡ä»¤è´¨é‡è¾ƒé«˜"
    elif score >= 0.7:
        return "ä¸€èˆ¬ - æŒ‡ä»¤è´¨é‡ä¸­ç­‰"
    elif score >= 0.6:
        return "éœ€è¦æ”¹è¿› - æŒ‡ä»¤è´¨é‡åä½"
    else:
        return "ä¸åˆæ ¼ - æŒ‡ä»¤è´¨é‡è¾ƒå·®ï¼Œéœ€è¦é‡æ–°è®¾è®¡"


def get_recommended_actions(quality_metrics: Dict[str, float]) -> List[str]:
    """è·å–æ¨èè¡ŒåŠ¨"""
    actions = []
    
    lowest_metric = min(quality_metrics.items(), key=lambda x: x[1])
    
    if lowest_metric[1] < 0.7:
        metric_actions = {
            "clarity": "é‡æ–°ç»„ç»‡è¯­è¨€ï¼Œä½¿ç”¨æ›´æ¸…æ™°çš„è¡¨è¾¾",
            "completeness": "è¡¥å……ç¼ºå¤±çš„å…³é”®ä¿¡æ¯",
            "specificity": "æ·»åŠ å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚",
            "actionability": "åˆ†è§£ä¸ºå¯æ‰§è¡Œçš„æ­¥éª¤",
            "risk_awareness": "è€ƒè™‘æ½œåœ¨çš„é£é™©å’Œé—®é¢˜"
        }
        actions.append(metric_actions[lowest_metric[0]])
    
    return actions if actions else ["ç»§ç»­ä¿æŒå½“å‰çš„æŒ‡ä»¤è´¨é‡"]



@mcp.tool()
def smart_programming_coach(
    user_request: str,
    project_context: str = "",
    mode: str = "full_guidance"
) -> str:
    """
    ğŸ“ æ™ºèƒ½ç¼–ç¨‹æ•™ç»ƒ - å¤§æ¨¡å‹çš„æ€ç»´å¯¼èˆªä»ª
    
    è¿™æ˜¯ä¸€ä¸ªå…ƒå·¥å…·ï¼Œä¸“é—¨æŒ‡å¯¼å¤§æ¨¡å‹å¦‚ä½•æ™ºèƒ½åœ°è¿ç”¨å…¶ä»–3ä¸ªå·¥å…·æ¥å®Œæˆå®Œæ•´çš„ç¼–ç¨‹æ€ç»´è¿‡ç¨‹ã€‚
    å®ƒä¼šæ ¹æ®ç”¨æˆ·è¯·æ±‚çš„ç‰¹ç‚¹ï¼Œè‡ªåŠ¨æ¨èæœ€ä½³çš„å·¥å…·ä½¿ç”¨ç­–ç•¥å’Œé¡ºåºã€‚
    
    **æ ¸å¿ƒä»·å€¼ï¼š**
    â€¢ è‡ªåŠ¨åˆ†æä»»åŠ¡ç‰¹ç‚¹ï¼Œæ¨èæœ€ä¼˜çš„å·¥å…·ä½¿ç”¨æµç¨‹
    â€¢ æä¾›å…·ä½“çš„å·¥å…·è°ƒç”¨ç¤ºä¾‹å’Œå‚æ•°å»ºè®®
    â€¢ ç¡®ä¿å¤§æ¨¡å‹èƒ½å¤Ÿç³»ç»Ÿæ€§åœ°å®Œæˆç¼–ç¨‹æ€ç»´è¿‡ç¨‹
    â€¢ é¿å…å·¥å…·ä½¿ç”¨çš„æ··ä¹±å’Œé—æ¼
    
    **ä½¿ç”¨æ¨¡å¼ï¼š**
    â€¢ **full_guidance**: å®Œæ•´æŒ‡å¯¼æ¨¡å¼ï¼Œæä¾›è¯¦ç»†çš„æ­¥éª¤å’Œå·¥å…·è°ƒç”¨ç¤ºä¾‹
    â€¢ **quick_start**: å¿«é€Ÿå…¥é—¨æ¨¡å¼ï¼Œæä¾›ç®€åŒ–çš„ä½¿ç”¨æµç¨‹
    â€¢ **expert_mode**: ä¸“å®¶æ¨¡å¼ï¼Œä»…æä¾›å…³é”®æç¤ºå’Œæœ€ä½³å®è·µ
    
    **æ™ºèƒ½æ¨èç­–ç•¥ï¼š**
    1. ç®€å•ä»»åŠ¡ â†’ ç›´æ¥ä½¿ç”¨ analyze_programming_context + validate_instruction_quality
    2. ä¸­ç­‰ä»»åŠ¡ â†’ å®Œæ•´3å·¥å…·æµç¨‹ï¼Œé‡ç‚¹åœ¨ guided_thinking_process
    3. å¤æ‚ä»»åŠ¡ â†’ è¿­ä»£å¼ä½¿ç”¨ï¼Œå¤šè½® guided_thinking_process æ·±åº¦æ€è€ƒ
    4. å­¦ä¹ åœºæ™¯ â†’ å®Œæ•´æµç¨‹ + è¯¦ç»†çš„æ€è€ƒè¿‡ç¨‹å±•ç¤º
    
    Args:
        user_request: ç”¨æˆ·çš„ç¼–ç¨‹è¯·æ±‚
        project_context: é¡¹ç›®ä¸Šä¸‹æ–‡ä¿¡æ¯
        mode: æŒ‡å¯¼æ¨¡å¼ ("full_guidance"/"quick_start"/"expert_mode")
    
    Returns:
        æ™ºèƒ½åŒ–çš„å·¥å…·ä½¿ç”¨æŒ‡å¯¼æ–¹æ¡ˆï¼ŒåŒ…å«ï¼š
        {
            "analysis": "ä»»åŠ¡åˆ†æå’Œå¤æ‚åº¦è¯„ä¼°",
            "recommended_workflow": "æ¨èçš„å·¥å…·ä½¿ç”¨æµç¨‹",
            "tool_sequence": ["å·¥å…·è°ƒç”¨é¡ºåº"],
            "sample_calls": {
                "step1": "å…·ä½“çš„å·¥å…·è°ƒç”¨ç¤ºä¾‹",
                "step2": "ä¸‹ä¸€æ­¥çš„è°ƒç”¨ç¤ºä¾‹"
            },
            "expected_outcomes": ["æ¯ä¸ªæ­¥éª¤çš„é¢„æœŸç»“æœ"],
            "tips": ["ä½¿ç”¨æŠ€å·§å’Œæ³¨æ„äº‹é¡¹"],
            "next_actions": "å»ºè®®çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨"
        }
    """
    
    # åˆ†æä»»åŠ¡ç‰¹å¾
    task_complexity = estimate_request_complexity(user_request)
    task_nature = analyze_request_nature(user_request)
    
    # æ ¹æ®å¤æ‚åº¦å’Œæ€§è´¨æ¨èæµç¨‹
    workflow = generate_workflow_recommendation(task_complexity, task_nature, mode)
    
    # ç”Ÿæˆå…·ä½“çš„å·¥å…·è°ƒç”¨ç¤ºä¾‹
    sample_calls = generate_sample_tool_calls(user_request, project_context, workflow)
    
    # æ„å»ºæŒ‡å¯¼æ–¹æ¡ˆ
    guidance = {
        "analysis": f"ä»»åŠ¡ç±»å‹: {task_nature}, å¤æ‚åº¦: {task_complexity}",
        "recommended_workflow": workflow["description"],
        "tool_sequence": workflow["sequence"],
        "sample_calls": sample_calls,
        "expected_outcomes": workflow["outcomes"],
        "tips": generate_usage_tips(task_complexity, mode),
        "next_actions": workflow["next_actions"]
    }
    
    return json.dumps(guidance, ensure_ascii=False, indent=2)


def estimate_request_complexity(user_request: str) -> str:
    """å¿«é€Ÿè¯„ä¼°è¯·æ±‚å¤æ‚åº¦"""
    request_lower = user_request.lower()
    
    # å¤æ‚åº¦æŒ‡æ ‡
    high_complexity_indicators = [
        "architecture", "system", "multiple", "integrate", "refactor", 
        "optimize", "æ¶æ„", "ç³»ç»Ÿ", "å¤šä¸ª", "é›†æˆ", "é‡æ„", "ä¼˜åŒ–"
    ]
    
    medium_complexity_indicators = [
        "feature", "function", "class", "module", "api",
        "åŠŸèƒ½", "å‡½æ•°", "ç±»", "æ¨¡å—", "æ¥å£"
    ]
    
    high_score = sum(1 for indicator in high_complexity_indicators if indicator in request_lower)
    medium_score = sum(1 for indicator in medium_complexity_indicators if indicator in request_lower)
    
    if high_score >= 2:
        return "complex"
    elif high_score >= 1 or medium_score >= 2:
        return "medium"
    else:
        return "simple"


def analyze_request_nature(user_request: str) -> str:
    """åˆ†æè¯·æ±‚æ€§è´¨"""
    request_lower = user_request.lower()
    
    if any(word in request_lower for word in ["learn", "understand", "explain", "å­¦ä¹ ", "ç†è§£", "è§£é‡Š"]):
        return "learning"
    elif any(word in request_lower for word in ["fix", "bug", "error", "ä¿®å¤", "é”™è¯¯", "é—®é¢˜"]):
        return "debugging"
    elif any(word in request_lower for word in ["optimize", "performance", "ä¼˜åŒ–", "æ€§èƒ½"]):
        return "optimization"
    elif any(word in request_lower for word in ["create", "implement", "build", "åˆ›å»º", "å®ç°", "æ„å»º"]):
        return "development"
    else:
        return "general"


def generate_workflow_recommendation(complexity: str, nature: str, mode: str) -> dict:
    """ç”Ÿæˆå·¥ä½œæµæ¨è"""
    
    workflows = {
        "simple": {
            "description": "è½»é‡çº§æµç¨‹ï¼šå¿«é€Ÿåˆ†æ + è´¨é‡éªŒè¯",
            "sequence": ["analyze_programming_context", "validate_instruction_quality"],
            "outcomes": ["è·å¾—ä»»åŠ¡åˆ†æå’Œæ€è€ƒæ¡†æ¶", "éªŒè¯æœ€ç»ˆæŒ‡ä»¤è´¨é‡"],
            "next_actions": "åŸºäºåˆ†æç»“æœç›´æ¥ç¼–å†™ç¼–ç¨‹æŒ‡ä»¤ï¼Œç„¶åéªŒè¯è´¨é‡"
        },
        "medium": {
            "description": "æ ‡å‡†æµç¨‹ï¼šå®Œæ•´çš„4é˜¶æ®µæ€è€ƒè¿‡ç¨‹",
            "sequence": ["analyze_programming_context", "guided_thinking_process(understanding)", 
                        "guided_thinking_process(planning)", "guided_thinking_process(implementation)",
                        "validate_instruction_quality"],
            "outcomes": ["ä»»åŠ¡åˆ†æ", "æ·±åº¦ç†è§£", "ç­–ç•¥è§„åˆ’", "å®ç°æŒ‡å¯¼", "è´¨é‡éªŒè¯"],
            "next_actions": "æŒ‰é˜¶æ®µé€æ­¥æ·±å…¥æ€è€ƒï¼Œæ¯ä¸ªé˜¶æ®µå……åˆ†æ€è€ƒåå†è¿›å…¥ä¸‹ä¸€é˜¶æ®µ"
        },
        "complex": {
            "description": "æ·±åº¦æµç¨‹ï¼šè¿­ä»£å¼æ€è€ƒ + å¤šè½®ä¼˜åŒ–",
            "sequence": ["analyze_programming_context", "guided_thinking_process(understanding)", 
                        "guided_thinking_process(planning)", "guided_thinking_process(implementation)",
                        "guided_thinking_process(validation)", "validate_instruction_quality",
                        "å¯èƒ½éœ€è¦å¤šè½®è¿­ä»£"],
            "outcomes": ["å…¨é¢åˆ†æ", "æ·±åº¦ç†è§£", "è¯¦ç»†è§„åˆ’", "ç²¾å‡†å®ç°", "ä¸¥æ ¼éªŒè¯", "é«˜è´¨é‡æŒ‡ä»¤"],
            "next_actions": "å®Œæˆä¸€è½®æ€è€ƒåï¼Œæ ¹æ®éœ€è¦è¿›è¡Œç¬¬äºŒè½®ä¼˜åŒ–æ€è€ƒ"
        }
    }
    
    base_workflow = workflows.get(complexity, workflows["medium"])
    
    # æ ¹æ®ä»»åŠ¡æ€§è´¨è°ƒæ•´
    if nature == "learning":
        base_workflow["description"] += " (æ³¨é‡æ€è€ƒè¿‡ç¨‹çš„å±•ç¤ºå’Œè§£é‡Š)"
    elif nature == "debugging":
        base_workflow["description"] += " (é‡ç‚¹å…³æ³¨é—®é¢˜æ ¹å› åˆ†æ)"
    elif nature == "optimization":
        base_workflow["description"] += " (å¼ºè°ƒæ€§èƒ½åˆ†æå’Œæƒè¡¡æ€è€ƒ)"
    
    return base_workflow


def generate_sample_tool_calls(user_request: str, project_context: str, workflow: dict) -> dict:
    """ç”Ÿæˆå…·ä½“çš„å·¥å…·è°ƒç”¨ç¤ºä¾‹"""
    
    samples = {}
    
    # ç¬¬ä¸€æ­¥ï¼šä»»åŠ¡åˆ†æ
    samples["step1_analyze"] = {
        "tool": "analyze_programming_context",
        "call": f'analyze_programming_context("{user_request}", "{project_context}")',
        "purpose": "è·å–ä»»åŠ¡åˆ†æå’Œæ€è€ƒæ¡†æ¶"
    }
    
    # å¦‚æœåŒ…å«guided_thinking_process
    if "guided_thinking_process" in str(workflow["sequence"]):
        samples["step2_understand"] = {
            "tool": "guided_thinking_process", 
            "call": 'guided_thinking_process(task_analysis_json, "understanding")',
            "purpose": "æ·±å…¥ç†è§£é˜¶æ®µçš„æ€è€ƒæŒ‡å¯¼",
            "note": "task_analysis_json æ˜¯ç¬¬ä¸€æ­¥è¿”å›çš„å®Œæ•´JSONç»“æœ"
        }
        
        samples["step3_plan"] = {
            "tool": "guided_thinking_process",
            "call": 'guided_thinking_process(task_analysis_json, "planning")', 
            "purpose": "è§„åˆ’é˜¶æ®µçš„æ€è€ƒæŒ‡å¯¼"
        }
        
        samples["step4_implement"] = {
            "tool": "guided_thinking_process",
            "call": 'guided_thinking_process(task_analysis_json, "implementation")',
            "purpose": "å®ç°é˜¶æ®µçš„æ€è€ƒæŒ‡å¯¼"
        }
    
    # æœ€åï¼šè´¨é‡éªŒè¯
    samples["final_validate"] = {
        "tool": "validate_instruction_quality",
        "call": 'validate_instruction_quality("your_final_instruction")',
        "purpose": "éªŒè¯æœ€ç»ˆç¼–ç¨‹æŒ‡ä»¤çš„è´¨é‡"
    }
    
    return samples


def generate_usage_tips(complexity: str, mode: str) -> list:
    """ç”Ÿæˆä½¿ç”¨æŠ€å·§"""
    
    base_tips = [
        "æ¯æ¬¡å·¥å…·è°ƒç”¨åï¼Œä»”ç»†é˜…è¯»è¿”å›ç»“æœå†è¿›è¡Œä¸‹ä¸€æ­¥",
        "æ€è€ƒè¿‡ç¨‹è¦å……åˆ†ï¼Œä¸è¦æ€¥äºå¾—å‡ºç»“è®º",
        "å°†å·¥å…·è¿”å›çš„JSONç»“æœå®Œæ•´ä¼ é€’ç»™ä¸‹ä¸€ä¸ªå·¥å…·"
    ]
    
    complexity_tips = {
        "simple": ["ä¿æŒç®€æ´ï¼Œé¿å…è¿‡åº¦åˆ†æ"],
        "medium": ["å¹³è¡¡æ·±åº¦å’Œæ•ˆç‡ï¼Œç¡®ä¿æ¯ä¸ªé˜¶æ®µéƒ½æœ‰æ”¶è·"],
        "complex": ["å…è®¸å¤šè½®è¿­ä»£ï¼Œå¤æ‚é—®é¢˜éœ€è¦æ—¶é—´æ¥æ€è€ƒ", "è€ƒè™‘åˆ†é˜¶æ®µå®ç°ç­–ç•¥"]
    }
    
    mode_tips = {
        "full_guidance": ["è¯¦ç»†è®°å½•æ¯æ­¥çš„æ€è€ƒè¿‡ç¨‹"],
        "quick_start": ["é‡ç‚¹å…³æ³¨æ ¸å¿ƒé—®é¢˜ï¼Œå¿«é€Ÿå½¢æˆæ–¹æ¡ˆ"],
        "expert_mode": ["ä¿¡ä»»è‡ªå·±çš„åˆ¤æ–­ï¼Œçµæ´»è°ƒæ•´æµç¨‹"]
    }
    
    return base_tips + complexity_tips.get(complexity, []) + mode_tips.get(mode, [])


@mcp.tool()
def session_manager(
    action: str = "list",
    session_id: str = ""
) -> str:
    """
    ğŸ—‚ï¸ ä¼šè¯ç®¡ç†å™¨ - æ™ºèƒ½ä¼šè¯çŠ¶æ€ç®¡ç†å·¥å…·
    
    **åŠŸèƒ½ç‰¹æ€§ï¼š**
    â€¢ ğŸ“‹ æŸ¥çœ‹æ´»è·ƒä¼šè¯åˆ—è¡¨
    â€¢ ğŸ” æ£€æŸ¥ç‰¹å®šä¼šè¯è¯¦æƒ…
    â€¢ ğŸ—‘ï¸ æ¸…ç†è¿‡æœŸæˆ–æ— ç”¨ä¼šè¯
    â€¢ ğŸ“Š åˆ†æä¼šè¯ä½¿ç”¨ç»Ÿè®¡
    â€¢ ğŸ”„ æ¢å¤ä¸­æ–­çš„æ€è€ƒæµç¨‹
    
    **æ”¯æŒçš„æ“ä½œï¼š**
    â€¢ **list**: åˆ—å‡ºæ‰€æœ‰æ´»è·ƒä¼šè¯
    â€¢ **detail**: æŸ¥çœ‹ç‰¹å®šä¼šè¯çš„è¯¦ç»†ä¿¡æ¯
    â€¢ **cleanup**: æ¸…ç†è¿‡æœŸä¼šè¯
    â€¢ **stats**: æ˜¾ç¤ºä½¿ç”¨ç»Ÿè®¡
    â€¢ **reset**: é‡ç½®ç‰¹å®šä¼šè¯çŠ¶æ€
    
    Args:
        action: æ“ä½œç±»å‹ ("list"/"detail"/"cleanup"/"stats"/"reset")
        session_id: ä¼šè¯IDï¼ˆæŸäº›æ“ä½œéœ€è¦ï¼‰
    
    Returns:
        æ“ä½œç»“æœçš„è¯¦ç»†ä¿¡æ¯
    """
    
    if action == "list":
        if not _session_cache:
            return json.dumps({
                "message": "å½“å‰æ²¡æœ‰æ´»è·ƒçš„ä¼šè¯",
                "suggestion": "ä½¿ç”¨ analyze_programming_context åˆ›å»ºæ–°ä¼šè¯"
            }, ensure_ascii=False, indent=2)
        
        sessions = []
        for sid, session in _session_cache.items():
            duration = int((time.time() - session.timestamp) / 60)
            sessions.append({
                "session_id": sid,
                "task_type": session.task_analysis.task_type.value,
                "complexity": session.task_analysis.complexity_level.value,
                "current_stage": session.current_stage,
                "progress": f"{len(session.stage_history)}/{len(session.thinking_frameworks)}",
                "duration_minutes": duration,
                "request_preview": session.user_request[:50] + "..." if len(session.user_request) > 50 else session.user_request
            })
        
        return json.dumps({
            "active_sessions": len(sessions),
            "sessions": sorted(sessions, key=lambda x: x['duration_minutes']),
            "usage_tip": "ä½¿ç”¨ session_manager('detail', 'session_id') æŸ¥çœ‹è¯¦æƒ…"
        }, ensure_ascii=False, indent=2)
    
    elif action == "detail":
        if not session_id:
            return json.dumps({"error": "éœ€è¦æä¾›session_id"}, ensure_ascii=False)
        
        if session_id not in _session_cache:
            return json.dumps({
                "error": "ä¼šè¯ä¸å­˜åœ¨",
                "available_sessions": list(_session_cache.keys())[-5:]
            }, ensure_ascii=False, indent=2)
        
        session = _session_cache[session_id]
        task_analysis = session.task_analysis
        
        detail = {
            "session_info": {
                "session_id": session_id,
                "created_time": time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(session.timestamp)),
                "duration_minutes": int((time.time() - session.timestamp) / 60),
                "current_stage": session.current_stage
            },
            "task_details": {
                "original_request": session.user_request,
                "task_type": task_analysis.task_type.value,
                "complexity_level": task_analysis.complexity_level.value,
                "core_objective": task_analysis.core_objective,
                "similarity_score": task_analysis.similarity_score
            },
            "progress_tracking": {
                "completed_stages": session.stage_history,
                "available_stages": list(session.thinking_frameworks.keys()),
                "progress_percentage": int((len(session.stage_history) / len(session.thinking_frameworks)) * 100),
                "next_recommended_stage": get_next_step(session.current_stage)
            },
            "quality_history": session.quality_scores,
            "learning_insights": task_analysis.learning_insights[:3] if task_analysis.learning_insights else [],
            "resume_suggestion": f"ç»§ç»­ä½¿ç”¨: guided_thinking_process('{session_id}', '{get_next_step(session.current_stage)}')"
        }
        
        return json.dumps(detail, ensure_ascii=False, indent=2)
    
    elif action == "cleanup":
        initial_count = len(_session_cache)
        cleanup_expired_sessions()
        cleaned_count = initial_count - len(_session_cache)
        
        return json.dumps({
            "cleanup_completed": True,
            "sessions_cleaned": cleaned_count,
            "remaining_sessions": len(_session_cache),
            "message": f"æ¸…ç†äº† {cleaned_count} ä¸ªè¿‡æœŸä¼šè¯"
        }, ensure_ascii=False, indent=2)
    
    elif action == "stats":
        if not _analysis_history:
            return json.dumps({
                "message": "æš‚æ— ä½¿ç”¨ç»Ÿè®¡æ•°æ®",
                "suggestion": "å¼€å§‹ä½¿ç”¨å·¥å…·åå°†äº§ç”Ÿç»Ÿè®¡æ•°æ®"
            }, ensure_ascii=False, indent=2)
        
        # ç»Ÿè®¡åˆ†æ
        task_types = {}
        complexities = {}
        for history in _analysis_history:
            task_type = history.get('task_type', 'unknown')
            complexity = history.get('complexity', 'unknown')
            task_types[task_type] = task_types.get(task_type, 0) + 1
            complexities[complexity] = complexities.get(complexity, 0) + 1
        
        # è®¡ç®—å¹³å‡è´¨é‡åˆ†æ•°
        all_quality_scores = []
        for session in _session_cache.values():
            all_quality_scores.extend(session.quality_scores.values())
        
        avg_quality = sum(all_quality_scores) / len(all_quality_scores) if all_quality_scores else 0
        
        stats = {
            "total_analyses": len(_analysis_history),
            "active_sessions": len(_session_cache),
            "task_type_distribution": task_types,
            "complexity_distribution": complexities,
            "average_quality_score": round(avg_quality, 2),
            "context_memory_entries": len(_context_memory),
            "most_common_task_type": max(task_types.items(), key=lambda x: x[1])[0] if task_types else "æ— ",
            "quality_assessments_performed": len(all_quality_scores)
        }
        
        return json.dumps(stats, ensure_ascii=False, indent=2)
    
    elif action == "reset":
        if not session_id:
            return json.dumps({"error": "éœ€è¦æä¾›session_id"}, ensure_ascii=False)
        
        if session_id not in _session_cache:
            return json.dumps({"error": "ä¼šè¯ä¸å­˜åœ¨"}, ensure_ascii=False)
        
        session = _session_cache[session_id]
        session.current_stage = "understanding"
        session.stage_history = []
        session.quality_scores = {}
        
        return json.dumps({
            "reset_completed": True,
            "session_id": session_id,
            "new_stage": "understanding",
            "message": "ä¼šè¯å·²é‡ç½®åˆ°åˆå§‹çŠ¶æ€",
            "next_action": f"ä½¿ç”¨ guided_thinking_process('{session_id}', 'understanding') é‡æ–°å¼€å§‹"
        }, ensure_ascii=False, indent=2)
    
    else:
        return json.dumps({
            "error": f"ä¸æ”¯æŒçš„æ“ä½œ: {action}",
            "supported_actions": ["list", "detail", "cleanup", "stats", "reset"]
        }, ensure_ascii=False, indent=2)


def main():
    """Main entry point to run the MCP server."""
    mcp.run()


if __name__ == "__main__":
    main()
